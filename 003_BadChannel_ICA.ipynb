{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92137d95",
   "metadata": {},
   "source": [
    "# EEG Preprocessing Pipeline: Bad Channel Detection & ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4b9b0",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "This notebook implements the third phase of EEG preprocessing pipeline with two main components:\n",
    "1. **Bad Channel Detection** - Automated identification of problematic channels\n",
    "2. **ICA Processing** - Independent Component Analysis for artifact removal\n",
    " \n",
    "### Pipeline Architecture\n",
    "```\n",
    "Filtered EEG ‚Üí Bad Channel Detection ‚Üí ICA ‚Üí Cleaned Data\n",
    "              ‚Üò Channel Reports     ‚Üò ICA Reports\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63d56f",
   "metadata": {},
   "source": [
    "# PHASE 3: BAD CHANNEL DETECTION & ICA\n",
    "\n",
    "**Purpose**\n",
    "- Automatically detect and handle bad EEG channels\n",
    "- Remove artifacts using Independent Component Analysis\n",
    "- Generate comprehensive quality reports for cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb893c28",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100d29a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bad Channel & ICA environment setup complete\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 1: Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import warnings\n",
    "from mne.preprocessing import ICA, corrmap, find_bad_channels_maxwell\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set professional plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Bad Channel & ICA environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2315741",
   "metadata": {},
   "source": [
    "## Step 2: Load Previous Pipeline Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3aa9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ LOADING FILTERED DATA INVENTORY\n",
      "==================================================\n",
      "üìä Filtered Data Overview:\n",
      "   ‚Ä¢ Total filtered files: 419\n",
      "   ‚Ä¢ Unique subjects: 27\n",
      "   ‚Ä¢ Unique sessions: 5\n",
      "\n",
      "üìã Sample filtered files:\n",
      "  subject_id session_id                              filename\n",
      "0     sub-01     follow  sub-01_follow_run-1_eeg_filtered.fif\n",
      "1     sub-01     follow  sub-01_follow_run-2_eeg_filtered.fif\n",
      "2     sub-01     follow  sub-01_follow_run-3_eeg_filtered.fif\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 2: Load filtered data and previous pipeline results\n",
    "def load_filtered_data_inventory(output_path):\n",
    "    \"\"\"\n",
    "    Load inventory of filtered EEG files from previous pipeline phase.\n",
    "    \n",
    "    Args:\n",
    "        output_path (Path): Main output directory path\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Inventory of filtered files with metadata\n",
    "    \"\"\"\n",
    "    print(\"üìÅ LOADING FILTERED DATA INVENTORY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    filtered_path = output_path / 'preprocessed_data' / 'raw_cleaned'\n",
    "    filtered_files = list(filtered_path.glob(\"*_filtered.fif\"))\n",
    "    \n",
    "    if not filtered_files:\n",
    "        raise FileNotFoundError(\"No filtered files found. Run filtering pipeline first.\")\n",
    "    \n",
    "    # Create inventory from filtered files\n",
    "    filtered_inventory = []\n",
    "    for file_path in filtered_files:\n",
    "        filename = file_path.stem  # Remove .fif extension\n",
    "        # Extract metadata from filename pattern\n",
    "        parts = filename.replace('_filtered', '').split('_')\n",
    "        \n",
    "        # Handle different filename patterns\n",
    "        if 'sub-' in filename and 'ses-' in filename:\n",
    "            subject_id = next((p for p in parts if p.startswith('sub-')), 'unknown')\n",
    "            session_id = next((p for p in parts if p.startswith('ses-')), 'unknown')\n",
    "            task_type = next((p for p in parts if p.startswith('task-')), 'eeg')\n",
    "        else:\n",
    "            # Fallback for custom naming\n",
    "            subject_id = parts[0] if len(parts) > 0 else 'unknown'\n",
    "            session_id = parts[1] if len(parts) > 1 else 'unknown'\n",
    "            task_type = parts[2] if len(parts) > 2 else 'eeg'\n",
    "        \n",
    "        filtered_inventory.append({\n",
    "            'subject_id': subject_id,\n",
    "            'session_id': session_id,\n",
    "            'task_type': task_type,\n",
    "            'filename': file_path.name,\n",
    "            'file_path': str(file_path),\n",
    "            'original_filename': filename.replace('_filtered', '')\n",
    "        })\n",
    "    \n",
    "    filtered_df = pd.DataFrame(filtered_inventory)\n",
    "    \n",
    "    print(f\"üìä Filtered Data Overview:\")\n",
    "    print(f\"   ‚Ä¢ Total filtered files: {len(filtered_df)}\")\n",
    "    print(f\"   ‚Ä¢ Unique subjects: {filtered_df['subject_id'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique sessions: {filtered_df['session_id'].nunique()}\")\n",
    "    \n",
    "    print(\"\\nüìã Sample filtered files:\")\n",
    "    print(filtered_df[['subject_id', 'session_id', 'filename']].head(3))\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Load filtered data inventory\n",
    "try:\n",
    "    filtered_inventory_df = load_filtered_data_inventory(Path('EEG_Preprocessing_Output'))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading filtered data: {e}\")\n",
    "    print(\"Please run the filtering pipeline first.\")\n",
    "    filtered_inventory_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdeb1a4",
   "metadata": {},
   "source": [
    "## Step 3: Bad Channel Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3640a625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING PROPERLY FIXED CORRELATION FUNCTION\n",
      "==================================================\n",
      "Testing Channel 0 (should have high correlation):\n",
      "   Calculated correlation: 0.575\n",
      "Testing Channel 4 (should have low correlation):\n",
      "   Calculated correlation: -0.020\n",
      "   Manual correlation check (0 vs 1): 0.994\n",
      "üéâ ‚úÖ CORRELATION FUNCTION PROPERLY FIXED!\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 3: PROPERLY FIXED Comprehensive Bad Channel Detection\n",
    "\n",
    "def calculate_channel_correlation(data, channel_idx):\n",
    "    \"\"\"\n",
    "    PROPERLY FIXED: Calculate how well a channel correlates with NEARBY channels.\n",
    "    This is the correct approach for EEG bad channel detection.\n",
    "    \"\"\"\n",
    "    channel_data = data[channel_idx]\n",
    "    \n",
    "    # For EEG, we care about correlation with NEARBY channels, not all channels\n",
    "    # This is more robust and physiologically meaningful\n",
    "    nearby_indices = get_nearby_channel_indices(channel_idx, data.shape[0])\n",
    "    \n",
    "    correlations = []\n",
    "    for i in nearby_indices:\n",
    "        if i == channel_idx:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            corr = np.corrcoef(channel_data, data[i])[0, 1]\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append(corr)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return np.mean(correlations) if correlations else 0.0\n",
    "\n",
    "def get_nearby_channel_indices(channel_idx, total_channels, max_neighbors=8):\n",
    "    \"\"\"\n",
    "    Get indices of nearby channels for correlation calculation.\n",
    "    For simplicity, we use adjacent channels.\n",
    "    \"\"\"\n",
    "    # Simple approach: take channels within a window around the target\n",
    "    start = max(0, channel_idx - max_neighbors // 2)\n",
    "    end = min(total_channels, channel_idx + max_neighbors // 2 + 1)\n",
    "    \n",
    "    nearby_indices = list(range(start, end))\n",
    "    \n",
    "    # Remove the target channel itself\n",
    "    if channel_idx in nearby_indices:\n",
    "        nearby_indices.remove(channel_idx)\n",
    "    \n",
    "    # Ensure we have some neighbors\n",
    "    if len(nearby_indices) < 2:\n",
    "        # Fallback: use all other channels\n",
    "        nearby_indices = [i for i in range(total_channels) if i != channel_idx]\n",
    "        nearby_indices = nearby_indices[:max_neighbors]  # Limit to reasonable number\n",
    "    \n",
    "    return nearby_indices\n",
    "\n",
    "def calculate_hurst_exponent(time_series):\n",
    "    \"\"\"Calculate Hurst exponent for channel data.\"\"\"\n",
    "    try:\n",
    "        lags = range(2, min(20, len(time_series)//10))\n",
    "        tau = [np.std(np.subtract(time_series[lag:], time_series[:-lag])) for lag in lags]\n",
    "        poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "        return poly[0]\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def calculate_line_noise_ratio(data, sfreq, line_freq=50):\n",
    "    \"\"\"Calculate line noise ratio at specified frequency.\"\"\"\n",
    "    try:\n",
    "        from scipy import signal\n",
    "        freqs, psd = signal.welch(data, sfreq, nperseg=min(1024, len(data)))\n",
    "        line_idx = np.argmin(np.abs(freqs - line_freq))\n",
    "        noise_band = [line_freq-2, line_freq+2]\n",
    "        noise_idx = (freqs >= noise_band[0]) & (freqs <= noise_band[1])\n",
    "        baseline_idx = (freqs >= line_freq-10) & (freqs <= line_freq-5) | (freqs >= line_freq+5) & (freqs <= line_freq+10)\n",
    "        \n",
    "        line_power = np.mean(psd[noise_idx])\n",
    "        baseline_power = np.mean(psd[baseline_idx])\n",
    "        \n",
    "        return line_power / baseline_power if baseline_power > 0 else 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "def calculate_adaptive_variance_threshold(channel_metrics):\n",
    "    \"\"\"\n",
    "    Calculate adaptive variance threshold based on data distribution.\n",
    "    \"\"\"\n",
    "    variances = [metrics['variance'] for metrics in channel_metrics.values()]\n",
    "    median_var = np.median(variances)\n",
    "    \n",
    "    # Conservative thresholds for clean EEG\n",
    "    high_threshold = median_var * 20  # Very conservative\n",
    "    low_threshold = median_var * 0.05  # Very conservative\n",
    "    \n",
    "    return {'high': high_threshold, 'low': low_threshold}\n",
    "\n",
    "def detect_bad_channels_statistical(raw, channel_metrics):\n",
    "    \"\"\"Detect bad channels using statistical thresholds.\"\"\"\n",
    "    bad_channels = []\n",
    "    \n",
    "    # Calculate adaptive thresholds\n",
    "    variances = [metrics['variance'] for metrics in channel_metrics.values()]\n",
    "    median_var = np.median(variances)\n",
    "    var_thresholds = calculate_adaptive_variance_threshold(channel_metrics)\n",
    "    \n",
    "    # Calculate correlation statistics\n",
    "    correlations = [metrics['correlation_with_others'] for metrics in channel_metrics.values()]\n",
    "    median_corr = np.median(correlations)\n",
    "    \n",
    "    print(f\"   üìä Variance - Median: {median_var:.1f} ¬µV¬≤, Thresholds: {var_thresholds['low']:.1f}-{var_thresholds['high']:.1f}\")\n",
    "    print(f\"   üìä Correlation - Median: {median_corr:.3f}, Range: {np.min(correlations):.3f}-{np.max(correlations):.3f}\")\n",
    "    \n",
    "    for ch_name, metrics in channel_metrics.items():\n",
    "        reasons = []\n",
    "        \n",
    "        # Check variance (too high or too low) - VERY CONSERVATIVE\n",
    "        if metrics['variance'] > var_thresholds['high']:\n",
    "            reasons.append(f\"high_var({metrics['variance']:.1f})\")\n",
    "        elif metrics['variance'] < var_thresholds['low']:\n",
    "            reasons.append(f\"low_var({metrics['variance']:.1f})\")\n",
    "            \n",
    "        # Check correlation - ADAPTIVE threshold based on data\n",
    "        corr_threshold = max(0.3, median_corr * 0.5)  # Adaptive threshold\n",
    "        if metrics['correlation_with_others'] < corr_threshold:\n",
    "            reasons.append(f\"low_corr({metrics['correlation_with_others']:.3f})\")\n",
    "            \n",
    "        # Check amplitude - CONSERVATIVE\n",
    "        if metrics['max_amplitude'] > 150:  # Conservative for clean EEG\n",
    "            reasons.append(f\"high_amp({metrics['max_amplitude']:.1f})\")\n",
    "        \n",
    "        # Only mark as bad if we have clear reasons\n",
    "        if reasons:\n",
    "            bad_channels.append(ch_name)\n",
    "            print(f\"      üö® {ch_name}: {', '.join(reasons)}\")\n",
    "    \n",
    "    return bad_channels\n",
    "\n",
    "def detect_bad_channels_comprehensive(raw, method='auto'):\n",
    "    \"\"\"\n",
    "    Detect bad channels using multiple criteria and methods.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Detecting bad channels using {method} method...\")\n",
    "    \n",
    "    # Get basic channel information\n",
    "    ch_names = raw.ch_names\n",
    "    data = raw.get_data() * 1e6  # Convert to ¬µV\n",
    "    \n",
    "    bad_channels_results = {\n",
    "        'method': method,\n",
    "        'total_channels': len(ch_names),\n",
    "        'channels_checked': ch_names,\n",
    "        'bad_channels_identified': [],\n",
    "        'detection_metrics': {},\n",
    "        'channel_quality_scores': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate channel quality metrics\n",
    "    print(\"   üìà Calculating channel metrics...\")\n",
    "    channel_metrics = {}\n",
    "    for i, ch_name in enumerate(ch_names):\n",
    "        ch_data = data[i]\n",
    "        \n",
    "        metrics = {\n",
    "            'variance': np.var(ch_data),\n",
    "            'mean_amplitude': np.mean(np.abs(ch_data)),\n",
    "            'max_amplitude': np.max(np.abs(ch_data)),\n",
    "            'hurst_exponent': calculate_hurst_exponent(ch_data),\n",
    "            'correlation_with_others': calculate_channel_correlation(data, i),\n",
    "            'line_noise_ratio': calculate_line_noise_ratio(ch_data, raw.info['sfreq'])\n",
    "        }\n",
    "        channel_metrics[ch_name] = metrics\n",
    "    \n",
    "    bad_channels_results['channel_quality_scores'] = channel_metrics\n",
    "    \n",
    "    # Use statistical method (most reliable)\n",
    "    bad_channels = detect_bad_channels_statistical(raw, channel_metrics)\n",
    "    bad_channels_results['bad_channels_identified'] = bad_channels\n",
    "    bad_channels_results['detection_metrics']['method'] = 'statistical'\n",
    "    \n",
    "    print(f\"   ‚úÖ Identified {len(bad_channels_results['bad_channels_identified'])} bad channels\")\n",
    "    \n",
    "    return bad_channels_results\n",
    "\n",
    "# Test the PROPERLY fixed function\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß™ TESTING PROPERLY FIXED CORRELATION FUNCTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create realistic test data - channels that should correlate\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    base_signal = np.random.randn(n_samples)\n",
    "    \n",
    "    test_data = np.zeros((5, n_samples))\n",
    "    test_data[0] = base_signal  # Channel 0\n",
    "    test_data[1] = base_signal * 0.9 + np.random.randn(n_samples) * 0.1  # Highly correlated\n",
    "    test_data[2] = base_signal * 0.8 + np.random.randn(n_samples) * 0.2  # Correlated  \n",
    "    test_data[3] = base_signal * 0.3 + np.random.randn(n_samples) * 0.7  # Weakly correlated\n",
    "    test_data[4] = np.random.randn(n_samples)  # Uncorrelated noise\n",
    "    \n",
    "    print(\"Testing Channel 0 (should have high correlation):\")\n",
    "    test_corr = calculate_channel_correlation(test_data, 0)\n",
    "    print(f\"   Calculated correlation: {test_corr:.3f}\")\n",
    "    \n",
    "    print(\"Testing Channel 4 (should have low correlation):\")\n",
    "    test_corr_noise = calculate_channel_correlation(test_data, 4)\n",
    "    print(f\"   Calculated correlation: {test_corr_noise:.3f}\")\n",
    "    \n",
    "    # Verify with manual calculation\n",
    "    manual_corr = np.corrcoef(test_data[0], test_data[1])[0, 1]\n",
    "    print(f\"   Manual correlation check (0 vs 1): {manual_corr:.3f}\")\n",
    "    \n",
    "    if test_corr > 0.5 and test_corr_noise < 0.5:\n",
    "        print(\"üéâ ‚úÖ CORRELATION FUNCTION PROPERLY FIXED!\")\n",
    "    else:\n",
    "        print(\"‚ùå Function still needs adjustment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634717f9",
   "metadata": {},
   "source": [
    "## Step 4: Apply Bad Channel Detection to Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58763bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ EXECUTING BAD CHANNEL DETECTION TEST...\n",
      "üß™ TESTING BAD CHANNEL DETECTION WITH FIXED FUNCTION\n",
      "============================================================\n",
      "üìÅ Testing with: sub-01_follow_run-1_eeg_filtered.fif\n",
      "üîÑ Loading filtered data...\n",
      "‚úÖ Loaded: 40 channels, 200.1s\n",
      "üîç VERIFYING CORRELATION FUNCTION\n",
      "   ‚Ä¢ Our function result: 0.775\n",
      "   ‚Ä¢ Manual average: 0.775\n",
      "   ‚Ä¢ Match: ‚úÖ\n",
      "\n",
      "üî¨ DATA DIAGNOSIS\n",
      "========================================\n",
      "üìä Data Statistics:\n",
      "   ‚Ä¢ Shape: (40, 50019)\n",
      "   ‚Ä¢ Global mean: -0.00 ¬µV\n",
      "   ‚Ä¢ Global std: 2.99 ¬µV\n",
      "   ‚Ä¢ Data range: -30.0 to 31.6 ¬µV\n",
      "   ‚Ä¢ Channel means range: -0.01 to 0.01 ¬µV\n",
      "\n",
      "üîç Quick Correlation Check:\n",
      "   ‚Ä¢ AF3: our=0.775, manual=0.694\n",
      "   ‚Ä¢ AF4: our=0.786, manual=0.824\n",
      "\n",
      "üîç Running bad channel detection...\n",
      "üîç Detecting bad channels using auto method...\n",
      "   üìà Calculating channel metrics...\n",
      "   üìä Variance - Median: 7.2 ¬µV¬≤, Thresholds: 0.4-144.7\n",
      "   üìä Correlation - Median: 0.461, Range: -0.112-0.843\n",
      "      üö® FC5: low_corr(0.270)\n",
      "      üö® C3: low_corr(0.243)\n",
      "      üö® C4: low_corr(0.268)\n",
      "      üö® C5: low_corr(-0.089)\n",
      "      üö® C6: low_corr(0.037)\n",
      "      üö® CP1: low_corr(0.161)\n",
      "      üö® CP2: low_corr(0.104)\n",
      "      üö® CP3: low_corr(0.069)\n",
      "      üö® CP4: low_corr(0.151)\n",
      "      üö® CP5: low_corr(0.151)\n",
      "      üö® CP6: low_corr(0.126)\n",
      "      üö® Pz: low_corr(0.298)\n",
      "      üö® CPz: low_corr(-0.112)\n",
      "   ‚úÖ Identified 13 bad channels\n",
      "\n",
      "üìä BAD CHANNEL DETECTION RESULTS:\n",
      "   ‚Ä¢ Method: auto\n",
      "   ‚Ä¢ Total channels: 40\n",
      "   ‚Ä¢ Bad channels identified: 13\n",
      "   ‚Ä¢ Bad channels: ['FC5', 'C3', 'C4', 'C5', 'C6', 'CP1', 'CP2', 'CP3', 'CP4', 'CP5', 'CP6', 'Pz', 'CPz']\n",
      "\n",
      "üìà CORRELATION STATISTICS:\n",
      "   ‚Ä¢ Range: -0.112 - 0.843\n",
      "   ‚Ä¢ Median: 0.461\n",
      "   ‚Ä¢ Mean: 0.443\n",
      "\n",
      "üìä SAMPLE CHANNEL METRICS (first 3):\n",
      "   AF3:\n",
      "      ‚Ä¢ Correlation: 0.775\n",
      "      ‚Ä¢ Variance: 7.1 ¬µV¬≤\n",
      "      ‚Ä¢ Max amplitude: 14.2 ¬µV\n",
      "   AF4:\n",
      "      ‚Ä¢ Correlation: 0.786\n",
      "      ‚Ä¢ Variance: 9.6 ¬µV¬≤\n",
      "      ‚Ä¢ Max amplitude: 29.4 ¬µV\n",
      "   Fz:\n",
      "      ‚Ä¢ Correlation: 0.816\n",
      "      ‚Ä¢ Variance: 11.0 ¬µV¬≤\n",
      "      ‚Ä¢ Max amplitude: 15.6 ¬µV\n",
      "\n",
      "üéâ TEST SUCCESSFUL! Ready for visualization.\n",
      "   ‚Ä¢ Identified 13 bad channels\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 4: Test Bad Channel Detection with FIXED Function\n",
    "\n",
    "def verify_correlation_function(raw):\n",
    "    \"\"\"Verify our correlation function is working correctly.\"\"\"\n",
    "    print(\"üîç VERIFYING CORRELATION FUNCTION\")\n",
    "    data = raw.get_data() * 1e6\n",
    "    \n",
    "    # Test our function vs manual calculation\n",
    "    test_channel = 0  # AF3\n",
    "    our_correlation = calculate_channel_correlation(data, test_channel)\n",
    "    \n",
    "    # Manual calculation for comparison\n",
    "    manual_correlations = []\n",
    "    for i in range(1, min(5, data.shape[0])):  # Compare with first 4 other channels\n",
    "        manual_corr = np.corrcoef(data[test_channel], data[i])[0, 1]\n",
    "        manual_correlations.append(manual_corr)\n",
    "    manual_avg = np.mean(manual_correlations)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Our function result: {our_correlation:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Manual average: {manual_avg:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Match: {'‚úÖ' if abs(our_correlation - manual_avg) < 0.2 else '‚ùå'}\")\n",
    "    \n",
    "    return our_correlation\n",
    "\n",
    "def diagnose_correlation_issue(raw):\n",
    "    \"\"\"\n",
    "    Diagnose channel correlations.\n",
    "    \"\"\"\n",
    "    print(\"\\nüî¨ DATA DIAGNOSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    data = raw.get_data() * 1e6\n",
    "    \n",
    "    # Check basic statistics\n",
    "    print(\"üìä Data Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Shape: {data.shape}\")\n",
    "    print(f\"   ‚Ä¢ Global mean: {np.mean(data):.2f} ¬µV\")\n",
    "    print(f\"   ‚Ä¢ Global std: {np.std(data):.2f} ¬µV\")\n",
    "    print(f\"   ‚Ä¢ Data range: {np.min(data):.1f} to {np.max(data):.1f} ¬µV\")\n",
    "    \n",
    "    # Check if data is already average referenced\n",
    "    channel_means = np.mean(data, axis=1)\n",
    "    print(f\"   ‚Ä¢ Channel means range: {np.min(channel_means):.2f} to {np.max(channel_means):.2f} ¬µV\")\n",
    "    \n",
    "    # Quick correlation check\n",
    "    print(f\"\\nüîç Quick Correlation Check:\")\n",
    "    for i in range(min(2, data.shape[0])):\n",
    "        our_corr = calculate_channel_correlation(data, i)\n",
    "        manual_corr = np.corrcoef(data[i], data[(i+1)%data.shape[0]])[0, 1]\n",
    "        print(f\"   ‚Ä¢ {raw.ch_names[i]}: our={our_corr:.3f}, manual={manual_corr:.3f}\")\n",
    "\n",
    "def test_bad_channel_detection(filtered_inventory_df):\n",
    "    \"\"\"\n",
    "    Test bad channel detection on a sample filtered file.\n",
    "    \"\"\"\n",
    "    print(\"üß™ TESTING BAD CHANNEL DETECTION WITH FIXED FUNCTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if len(filtered_inventory_df) == 0:\n",
    "        print(\"‚ùå No filtered files available for testing\")\n",
    "        return None\n",
    "    \n",
    "    # Use first available file\n",
    "    sample_file = filtered_inventory_df.iloc[0]\n",
    "    print(f\"üìÅ Testing with: {sample_file['filename']}\")\n",
    "    \n",
    "    try:\n",
    "        # Load filtered data\n",
    "        print(\"üîÑ Loading filtered data...\")\n",
    "        raw_filtered = mne.io.read_raw_fif(sample_file['file_path'], preload=True, verbose=False)\n",
    "        print(f\"‚úÖ Loaded: {len(raw_filtered.ch_names)} channels, {raw_filtered.times[-1]:.1f}s\")\n",
    "        \n",
    "        # VERIFY OUR FUNCTION FIRST\n",
    "        verify_correlation_function(raw_filtered)\n",
    "        \n",
    "        # RUN DIAGNOSIS\n",
    "        diagnose_correlation_issue(raw_filtered)\n",
    "        \n",
    "        # Detect bad channels\n",
    "        print(\"\\nüîç Running bad channel detection...\")\n",
    "        bad_channels_results = detect_bad_channels_comprehensive(raw_filtered, method='auto')\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìä BAD CHANNEL DETECTION RESULTS:\")\n",
    "        print(f\"   ‚Ä¢ Method: {bad_channels_results['method']}\")\n",
    "        print(f\"   ‚Ä¢ Total channels: {bad_channels_results['total_channels']}\")\n",
    "        print(f\"   ‚Ä¢ Bad channels identified: {len(bad_channels_results['bad_channels_identified'])}\")\n",
    "        \n",
    "        if bad_channels_results['bad_channels_identified']:\n",
    "            print(f\"   ‚Ä¢ Bad channels: {bad_channels_results['bad_channels_identified']}\")\n",
    "        else:\n",
    "            print(\"   ‚Ä¢ No bad channels detected - data looks clean! ‚úÖ\")\n",
    "        \n",
    "        # Show correlation statistics\n",
    "        correlations = [metrics['correlation_with_others'] for metrics in bad_channels_results['channel_quality_scores'].values()]\n",
    "        print(f\"\\nüìà CORRELATION STATISTICS:\")\n",
    "        print(f\"   ‚Ä¢ Range: {np.min(correlations):.3f} - {np.max(correlations):.3f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {np.median(correlations):.3f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {np.mean(correlations):.3f}\")\n",
    "        \n",
    "        # Show sample channel metrics\n",
    "        print(f\"\\nüìä SAMPLE CHANNEL METRICS (first 3):\")\n",
    "        for i, (ch_name, metrics) in enumerate(list(bad_channels_results['channel_quality_scores'].items())[:3]):\n",
    "            print(f\"   {ch_name}:\")\n",
    "            print(f\"      ‚Ä¢ Correlation: {metrics['correlation_with_others']:.3f}\")\n",
    "            print(f\"      ‚Ä¢ Variance: {metrics['variance']:.1f} ¬µV¬≤\")\n",
    "            print(f\"      ‚Ä¢ Max amplitude: {metrics['max_amplitude']:.1f} ¬µV\")\n",
    "        \n",
    "        return raw_filtered, bad_channels_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in bad channel detection: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# RUN THE TEST\n",
    "print(\"üöÄ EXECUTING BAD CHANNEL DETECTION TEST...\")\n",
    "sample_raw, bad_ch_results = test_bad_channel_detection(filtered_inventory_df)\n",
    "\n",
    "if sample_raw is not None and bad_ch_results is not None:\n",
    "    print(f\"\\nüéâ TEST SUCCESSFUL! Ready for visualization.\")\n",
    "    print(f\"   ‚Ä¢ Identified {len(bad_ch_results['bad_channels_identified'])} bad channels\")\n",
    "else:\n",
    "    print(f\"\\nüí• TEST FAILED! Check error above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bb1fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DETAILED BAD CHANNEL ANALYSIS\n",
      "==================================================\n",
      "Identified 13 bad channels:\n",
      "   FC5: low_corr(0.270 vs median 0.461)\n",
      "   C3: low_corr(0.243 vs median 0.461)\n",
      "   C4: low_corr(0.268 vs median 0.461)\n",
      "   C5: low_corr(-0.089 vs median 0.461)\n",
      "   C6: low_corr(0.037 vs median 0.461)\n",
      "   CP1: low_corr(0.161 vs median 0.461)\n",
      "   CP2: low_corr(0.104 vs median 0.461)\n",
      "   CP3: low_corr(0.069 vs median 0.461)\n",
      "   CP4: low_corr(0.151 vs median 0.461)\n",
      "   CP5: low_corr(0.151 vs median 0.461)\n",
      "   CP6: low_corr(0.126 vs median 0.461)\n",
      "   Pz: low_corr(0.298 vs median 0.461)\n",
      "   CPz: low_corr(-0.112 vs median 0.461)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.5: to investigate the 13 bad channels:\n",
    "\n",
    "def diagnose_bad_channel_reasons(bad_ch_results):\n",
    "    \"\"\"Detailed analysis of why channels were marked bad\"\"\"\n",
    "    print(\"üîç DETAILED BAD CHANNEL ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    bad_channels = bad_ch_results['bad_channels_identified']\n",
    "    metrics = bad_ch_results['channel_quality_scores']\n",
    "    \n",
    "    print(f\"Identified {len(bad_channels)} bad channels:\")\n",
    "    \n",
    "    for ch_name in bad_channels:\n",
    "        ch_metrics = metrics[ch_name]\n",
    "        reasons = []\n",
    "        \n",
    "        # Check each criterion\n",
    "        variances = [m['variance'] for m in metrics.values()]\n",
    "        median_var = np.median(variances)\n",
    "        \n",
    "        if ch_metrics['variance'] > median_var * 20:\n",
    "            reasons.append(f\"high_var({ch_metrics['variance']:.1f} vs median {median_var:.1f})\")\n",
    "        elif ch_metrics['variance'] < median_var * 0.05:\n",
    "            reasons.append(f\"low_var({ch_metrics['variance']:.1f} vs median {median_var:.1f})\")\n",
    "            \n",
    "        correlations = [m['correlation_with_others'] for m in metrics.values()]\n",
    "        median_corr = np.median(correlations)\n",
    "        \n",
    "        if ch_metrics['correlation_with_others'] < max(0.3, median_corr * 0.5):\n",
    "            reasons.append(f\"low_corr({ch_metrics['correlation_with_others']:.3f} vs median {median_corr:.3f})\")\n",
    "            \n",
    "        if ch_metrics['max_amplitude'] > 150:\n",
    "            reasons.append(f\"high_amp({ch_metrics['max_amplitude']:.1f})\")\n",
    "            \n",
    "        print(f\"   {ch_name}: {', '.join(reasons)}\")\n",
    "\n",
    "# Run this diagnosis\n",
    "if bad_ch_results is not None:\n",
    "    diagnose_bad_channel_reasons(bad_ch_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this diagnostic function to Cell 4 before the test\n",
    "def diagnose_correlation_issue(raw):\n",
    "    \"\"\"\n",
    "    Diagnose why channel correlations are so low.\n",
    "    \"\"\"\n",
    "    print(\"\\nüî¨ CORRELATION DIAGNOSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    data = raw.get_data() * 1e6\n",
    "    \n",
    "    # Check basic statistics\n",
    "    print(\"üìä Data Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Shape: {data.shape}\")\n",
    "    print(f\"   ‚Ä¢ Global mean: {np.mean(data):.2f} ¬µV\")\n",
    "    print(f\"   ‚Ä¢ Global std: {np.std(data):.2f} ¬µV\")\n",
    "    print(f\"   ‚Ä¢ Data range: {np.min(data):.1f} to {np.max(data):.1f} ¬µV\")\n",
    "    \n",
    "    # Check if data is already average referenced\n",
    "    channel_means = np.mean(data, axis=1)\n",
    "    print(f\"   ‚Ä¢ Channel means range: {np.min(channel_means):.2f} to {np.max(channel_means):.2f} ¬µV\")\n",
    "    \n",
    "    # Manual correlation check\n",
    "    print(f\"\\nüîç Manual Correlation Check (first 3 channels):\")\n",
    "    for i in range(min(3, data.shape[0])):\n",
    "        for j in range(i+1, min(4, data.shape[0])):\n",
    "            corr = np.corrcoef(data[i], data[j])[0,1]\n",
    "            print(f\"   ‚Ä¢ {raw.ch_names[i]} vs {raw.ch_names[j]}: {corr:.3f}\")\n",
    "    \n",
    "    # Check reference\n",
    "    print(f\"\\nüìã Reference Info:\")\n",
    "    print(f\"   ‚Ä¢ Reference: {getattr(raw.info, 'custom_ref_applied', 'Unknown')}\")\n",
    "    print(f\"   ‚Ä¢ Description: {raw.info.get('description', 'Not specified')}\")\n",
    "\n",
    "# Then modify the test function to include diagnosis:\n",
    "def test_bad_channel_detection(filtered_inventory_df):\n",
    "    \"\"\"\n",
    "    Test bad channel detection on a sample filtered file.\n",
    "    \"\"\"\n",
    "    print(\"üß™ TESTING BAD CHANNEL DETECTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if len(filtered_inventory_df) == 0:\n",
    "        print(\"‚ùå No filtered files available for testing\")\n",
    "        return None\n",
    "    \n",
    "    # Use first available file\n",
    "    sample_file = filtered_inventory_df.iloc[0]\n",
    "    print(f\"üìÅ Testing with: {sample_file['filename']}\")\n",
    "    print(f\"üìÅ File path: {sample_file['file_path']}\")\n",
    "    \n",
    "    try:\n",
    "        # Load filtered data\n",
    "        print(\"üîÑ Loading filtered data...\")\n",
    "        raw_filtered = mne.io.read_raw_fif(sample_file['file_path'], preload=True, verbose=False)\n",
    "        print(f\"‚úÖ Loaded: {len(raw_filtered.ch_names)} channels, {raw_filtered.times[-1]:.1f}s\")\n",
    "        \n",
    "        # RUN DIAGNOSIS FIRST\n",
    "        diagnose_correlation_issue(raw_filtered)\n",
    "        \n",
    "        # Detect bad channels (with relaxed thresholds for now)\n",
    "        print(\"\\nüîç Running bad channel detection...\")\n",
    "        bad_channels_results = detect_bad_channels_comprehensive(raw_filtered, method='auto')\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìä BAD CHANNEL DETECTION RESULTS:\")\n",
    "        print(f\"   ‚Ä¢ Method: {bad_channels_results['method']}\")\n",
    "        print(f\"   ‚Ä¢ Total channels: {bad_channels_results['total_channels']}\")\n",
    "        print(f\"   ‚Ä¢ Bad channels identified: {len(bad_channels_results['bad_channels_identified'])}\")\n",
    "        \n",
    "        if bad_channels_results['bad_channels_identified']:\n",
    "            print(f\"   ‚Ä¢ Bad channels: {bad_channels_results['bad_channels_identified']}\")\n",
    "        else:\n",
    "            print(\"   ‚Ä¢ No bad channels detected\")\n",
    "        \n",
    "        return raw_filtered, bad_channels_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in bad channel detection: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# ACTUALLY RUN THE TEST\n",
    "print(\"üöÄ EXECUTING BAD CHANNEL DETECTION TEST...\")\n",
    "sample_raw, bad_ch_results = test_bad_channel_detection(filtered_inventory_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032ad59",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Bad Channel Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457fd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Create Bad Channel Visualization\n",
    "def create_bad_channel_visualization(raw, bad_channels_results, file_info, output_path):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization for bad channel detection results.\n",
    "    \n",
    "    Args:\n",
    "        raw: MNE Raw object\n",
    "        bad_channels_results: Bad channel detection results\n",
    "        file_info: File metadata\n",
    "        output_path: Output directory path\n",
    "    \"\"\"\n",
    "    print(\"üìà CREATING BAD CHANNEL VISUALIZATION\")\n",
    "    \n",
    "    # Use non-interactive backend\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Bad Channel Detection: {file_info[\"subject_id\"]} - {file_info[\"session_id\"]}', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    data = raw.get_data() * 1e6\n",
    "    ch_names = raw.ch_names\n",
    "    bad_channels = bad_channels_results['bad_channels_identified']\n",
    "    channel_metrics = bad_channels_results['channel_quality_scores']\n",
    "    \n",
    "    # 1. Channel variances with bad channels highlighted\n",
    "    ax1 = axes[0, 0]\n",
    "    variances = [channel_metrics[ch]['variance'] for ch in ch_names]\n",
    "    colors = ['red' if ch in bad_channels else 'skyblue' for ch in ch_names]\n",
    "    \n",
    "    bars = ax1.bar(range(len(variances)), variances, color=colors, alpha=0.7)\n",
    "    ax1.set_title('Channel Variances (Red = Bad Channels)')\n",
    "    ax1.set_xlabel('Channel Index')\n",
    "    ax1.set_ylabel('Variance (¬µV¬≤)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Channel correlation matrix\n",
    "    ax2 = axes[0, 1]\n",
    "    try:\n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = np.corrcoef(data)\n",
    "        im = ax2.imshow(corr_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "        \n",
    "        # Mark bad channels\n",
    "        bad_indices = [i for i, ch in enumerate(ch_names) if ch in bad_channels]\n",
    "        for idx in bad_indices:\n",
    "            ax2.axhline(idx - 0.5, color='red', linewidth=2)\n",
    "            ax2.axvline(idx - 0.5, color='red', linewidth=2)\n",
    "        \n",
    "        ax2.set_title('Channel Correlation Matrix\\n(Red Lines = Bad Channels)')\n",
    "        ax2.set_xlabel('Channel Index')\n",
    "        ax2.set_ylabel('Channel Index')\n",
    "        plt.colorbar(im, ax=ax2, shrink=0.6)\n",
    "    except Exception as e:\n",
    "        ax2.text(0.5, 0.5, f'Correlation matrix failed: {str(e)}', \n",
    "                transform=ax2.transAxes, ha='center')\n",
    "        ax2.set_title('Channel Correlation Matrix')\n",
    "    \n",
    "    # 3. Channel quality scatter plot\n",
    "    ax3 = axes[1, 0]\n",
    "    variances = [channel_metrics[ch]['variance'] for ch in ch_names]\n",
    "    correlations = [channel_metrics[ch]['correlation_with_others'] for ch in ch_names]\n",
    "    \n",
    "    colors = ['red' if ch in bad_channels else 'blue' for ch in ch_names]\n",
    "    sizes = [100 if ch in bad_channels else 50 for ch in ch_names]\n",
    "    \n",
    "    scatter = ax3.scatter(variances, correlations, c=colors, s=sizes, alpha=0.7)\n",
    "    ax3.set_title('Channel Quality: Variance vs Correlation')\n",
    "    ax3.set_xlabel('Variance (¬µV¬≤)')\n",
    "    ax3.set_ylabel('Mean Correlation')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add thresholds\n",
    "    median_var = np.median(variances)\n",
    "    ax3.axvline(median_var * 5, color='red', linestyle='--', alpha=0.5, label='High var threshold')\n",
    "    ax3.axvline(median_var * 0.01, color='orange', linestyle='--', alpha=0.5, label='Low var threshold')\n",
    "    ax3.axhline(0.4, color='green', linestyle='--', alpha=0.5, label='Low corr threshold')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Bad channel locations\n",
    "    ax4 = axes[1, 1]\n",
    "    try:\n",
    "        from mne.viz import plot_sensors\n",
    "        plot_sensors(raw.info, show_names=True, axes=ax4, show=False)\n",
    "        \n",
    "        # Highlight bad channels\n",
    "        if bad_channels:\n",
    "            bad_ch_idx = [raw.ch_names.index(ch) for ch in bad_channels]\n",
    "            ax4.scatter([], [], color='red', s=100, label='Bad channels')  # For legend\n",
    "            # Note: Sensor positions would need to be accessed for precise highlighting\n",
    "        ax4.set_title('Channel Locations\\n(Red = Bad Channels)')\n",
    "        ax4.legend()\n",
    "    except Exception as e:\n",
    "        ax4.text(0.5, 0.5, f'Sensor plot error: {str(e)}', \n",
    "                transform=ax4.transAxes, ha='center')\n",
    "        ax4.set_title('Channel Locations')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    original_name = file_info['original_filename']\n",
    "    fig_path = output_path / 'preprocessed_data' / 'visualizations' / f'{original_name}_bad_channels.png'\n",
    "    fig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"‚úÖ Bad channel visualization saved: {fig_path.name}\")\n",
    "    \n",
    "    # Switch back to interactive backend\n",
    "    matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "    \n",
    "    return fig_path\n",
    "\n",
    "# Create visualization if test was successful\n",
    "if sample_raw is not None and bad_ch_results is not None:\n",
    "    sample_file_info = filtered_inventory_df.iloc[0]\n",
    "    bad_ch_viz_path = create_bad_channel_visualization(\n",
    "        sample_raw, bad_ch_results, sample_file_info, Path('EEG_Preprocessing_Output')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf6ac4e",
   "metadata": {},
   "source": [
    "## Step 6: ICA Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a597d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: ICA Processing Implementation\n",
    "def perform_ica_processing(raw, n_components=0.95, method='fastica', random_state=42):\n",
    "    \"\"\"\n",
    "    Perform Independent Component Analysis on EEG data.\n",
    "    \n",
    "    Args:\n",
    "        raw: MNE Raw object\n",
    "        n_components: Number of components (float for variance, int for exact)\n",
    "        method: ICA method ('fastica', 'infomax', 'picard')\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ICA object, ICA results dictionary)\n",
    "    \"\"\"\n",
    "    print(\"üß† PERFORMING INDEPENDENT COMPONENT ANALYSIS\")\n",
    "    print(f\"   ‚Ä¢ Method: {method}\")\n",
    "    print(f\"   ‚Ä¢ Components: {n_components}\")\n",
    "    print(f\"   ‚Ä¢ Random state: {random_state}\")\n",
    "    \n",
    "    # Create and fit ICA\n",
    "    ica = ICA(\n",
    "        n_components=n_components,\n",
    "        method=method,\n",
    "        random_state=random_state,\n",
    "        max_iter=800,\n",
    "        fit_params=dict(extended=True) if method == 'infomax' else None\n",
    "    )\n",
    "    \n",
    "    # Fit ICA\n",
    "    ica.fit(raw, verbose=False)\n",
    "    \n",
    "    # Analyze components\n",
    "    ica_results = {\n",
    "        'n_components': ica.n_components_,\n",
    "        'method': method,\n",
    "        'explained_variance': ica.pca_explained_variance_ratio_.sum(),\n",
    "        'component_characteristics': analyze_ica_components(ica, raw),\n",
    "        'fitting_time': 'N/A'  # Could be enhanced with timing\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ ICA completed: {ica.n_components_} components\")\n",
    "    print(f\"   ‚Ä¢ Explained variance: {ica_results['explained_variance']:.3f}\")\n",
    "    \n",
    "    return ica, ica_results\n",
    "\n",
    "def analyze_ica_components(ica, raw):\n",
    "    \"\"\"\n",
    "    Analyze ICA components for artifact characteristics.\n",
    "    \n",
    "    Args:\n",
    "        ica: Fitted ICA object\n",
    "        raw: Original raw data\n",
    "        \n",
    "    Returns:\n",
    "        dict: Component analysis results\n",
    "    \"\"\"\n",
    "    component_analysis = {}\n",
    "    \n",
    "    for idx in range(ica.n_components_):\n",
    "        # Get component properties\n",
    "        component_data = ica.get_components()[:, idx]\n",
    "        \n",
    "        analysis = {\n",
    "            'variance_explained': ica.pca_explained_variance_ratio_[idx] if idx < len(ica.pca_explained_variance_ratio_) else 0,\n",
    "            'max_amplitude': np.max(np.abs(component_data)),\n",
    "            'topographic_std': np.std(component_data),\n",
    "            'is_eyeblink_likely': is_eyeblink_component(component_data, ica, idx),\n",
    "            'is_cardiac_likely': is_cardiac_component(component_data, raw.info['sfreq']),\n",
    "            'is_noise_likely': is_noise_component(component_data)\n",
    "        }\n",
    "        \n",
    "        component_analysis[f'component_{idx:02d}'] = analysis\n",
    "    \n",
    "    return component_analysis\n",
    "\n",
    "def is_eyeblink_component(component_data, ica, component_idx):\n",
    "    \"\"\"Heuristic for eyeblink component detection.\"\"\"\n",
    "    # Frontal channels typically have high weights for eyeblinks\n",
    "    frontal_channels = ['Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8']\n",
    "    frontal_indices = [i for i, ch in enumerate(ica.ch_names) if ch in frontal_channels]\n",
    "    \n",
    "    if not frontal_indices:\n",
    "        return False\n",
    "    \n",
    "    frontal_weights = np.abs(component_data[frontal_indices])\n",
    "    max_frontal = np.max(frontal_weights) if len(frontal_weights) > 0 else 0\n",
    "    max_overall = np.max(np.abs(component_data))\n",
    "    \n",
    "    return max_frontal / max_overall > 0.5 if max_overall > 0 else False\n",
    "\n",
    "def is_cardiac_component(component_data, sfreq):\n",
    "    \"\"\"Heuristic for cardiac component detection.\"\"\"\n",
    "    # Simple variance-based heuristic\n",
    "    return np.var(component_data) > np.median(np.var(component_data)) * 2\n",
    "\n",
    "def is_noise_component(component_data):\n",
    "    \"\"\"Heuristic for noise component detection.\"\"\"\n",
    "    # High kurtosis often indicates noise\n",
    "    from scipy.stats import kurtosis\n",
    "    return kurtosis(component_data) > 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916214b2",
   "metadata": {},
   "source": [
    "## Step 7: Test ICA Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e5a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Test ICA Processing on Sample File\n",
    "def test_ica_processing(raw, file_info):\n",
    "    \"\"\"\n",
    "    Test ICA processing on a sample file.\n",
    "    \n",
    "    Args:\n",
    "        raw: MNE Raw object\n",
    "        file_info: File metadata\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ICA object, ICA results)\n",
    "    \"\"\"\n",
    "    print(\"üß™ TESTING ICA PROCESSING\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìÅ Processing: {file_info['filename']}\")\n",
    "    \n",
    "    try:\n",
    "        # Perform ICA\n",
    "        ica, ica_results = perform_ica_processing(raw, n_components=0.95, method='fastica')\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìä ICA PROCESSING RESULTS:\")\n",
    "        print(f\"   ‚Ä¢ Components extracted: {ica_results['n_components']}\")\n",
    "        print(f\"   ‚Ä¢ Total variance explained: {ica_results['explained_variance']:.3f}\")\n",
    "        print(f\"   ‚Ä¢ Method: {ica_results['method']}\")\n",
    "        \n",
    "        # Component analysis summary\n",
    "        component_chars = ica_results['component_characteristics']\n",
    "        eyeblink_components = [comp for comp, chars in component_chars.items() \n",
    "                              if chars['is_eyeblink_likely']]\n",
    "        cardiac_components = [comp for comp, chars in component_chars.items() \n",
    "                             if chars['is_cardiac_likely']]\n",
    "        noise_components = [comp for comp, chars in component_chars.items() \n",
    "                           if chars['is_noise_likely']]\n",
    "        \n",
    "        print(f\"\\nüìà COMPONENT ANALYSIS:\")\n",
    "        print(f\"   ‚Ä¢ Potential eyeblink components: {len(eyeblink_components)}\")\n",
    "        print(f\"   ‚Ä¢ Potential cardiac components: {len(cardiac_components)}\")\n",
    "        print(f\"   ‚Ä¢ Potential noise components: {len(noise_components)}\")\n",
    "        \n",
    "        if eyeblink_components:\n",
    "            print(f\"   ‚Ä¢ Eyeblink components: {eyeblink_components[:3]}...\")\n",
    "        \n",
    "        return ica, ica_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in ICA processing: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Test ICA processing if sample data is available\n",
    "if sample_raw is not None:\n",
    "    sample_file_info = filtered_inventory_df.iloc[0]\n",
    "    ica_obj, ica_results = test_ica_processing(sample_raw, sample_file_info)\n",
    "else:\n",
    "    ica_obj, ica_results = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a9bda",
   "metadata": {},
   "source": [
    "## Step 8: Visualize ICA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256db53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 8: Create ICA Visualization\n",
    "def create_ica_visualization(raw, ica, ica_results, file_info, output_path, n_components_show=8):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization for ICA results.\n",
    "    \n",
    "    Args:\n",
    "        raw: MNE Raw object\n",
    "        ica: Fitted ICA object\n",
    "        ica_results: ICA analysis results\n",
    "        file_info: File metadata\n",
    "        output_path: Output directory path\n",
    "        n_components_show: Number of components to display\n",
    "    \"\"\"\n",
    "    print(\"üìà CREATING ICA VISUALIZATION\")\n",
    "    \n",
    "    # Use non-interactive backend\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    n_components = min(ica.n_components_, n_components_show)\n",
    "    n_rows = (n_components + 1) // 2  # Adjust layout\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(15, 4 * n_rows))\n",
    "    fig.suptitle(f'ICA Components: {file_info[\"subject_id\"]} - {file_info[\"session_id\"]}\\n'\n",
    "                 f'{ica.n_components_} components, {ica_results[\"explained_variance\"]:.3f} variance explained',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Flatten axes for easier indexing\n",
    "    if n_rows > 1:\n",
    "        axes_flat = axes.flatten()\n",
    "    else:\n",
    "        axes_flat = [axes] if n_components == 1 else axes\n",
    "    \n",
    "    # Plot each component\n",
    "    for idx in range(n_components):\n",
    "        ax = axes_flat[idx]\n",
    "        \n",
    "        try:\n",
    "            # Plot component topography\n",
    "            ica.plot_components(picks=[idx], axes=ax, show=False)\n",
    "            ax.set_title(f'Component {idx}', fontweight='bold')\n",
    "            \n",
    "            # Add component type annotation\n",
    "            comp_key = f'component_{idx:02d}'\n",
    "            comp_chars = ica_results['component_characteristics'].get(comp_key, {})\n",
    "            \n",
    "            component_type = []\n",
    "            if comp_chars.get('is_eyeblink_likely'):\n",
    "                component_type.append('üëÅÔ∏è')\n",
    "            if comp_chars.get('is_cardiac_likely'):\n",
    "                component_type.append('‚ù§Ô∏è')\n",
    "            if comp_chars.get('is_noise_likely'):\n",
    "                component_type.append('üì¢')\n",
    "            \n",
    "            if component_type:\n",
    "                ax.text(0.02, 0.98, ' '.join(component_type), \n",
    "                       transform=ax.transAxes, fontsize=12,\n",
    "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                       \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f'Component {idx}\\nPlot failed', \n",
    "                   transform=ax.transAxes, ha='center', va='center')\n",
    "            ax.set_title(f'Component {idx}')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_components, len(axes_flat)):\n",
    "        axes_flat[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    original_name = file_info['original_filename']\n",
    "    fig_path = output_path / 'preprocessed_data' / 'visualizations' / f'{original_name}_ica_components.png'\n",
    "    fig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"‚úÖ ICA visualization saved: {fig_path.name}\")\n",
    "    \n",
    "    # Switch back to interactive backend\n",
    "    matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "    \n",
    "    return fig_path\n",
    "\n",
    "# Create ICA visualization if available\n",
    "if ica_obj is not None and ica_results is not None:\n",
    "    ica_viz_path = create_ica_visualization(\n",
    "        sample_raw, ica_obj, ica_results, sample_file_info, Path('EEG_Preprocessing_Output')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ae7d1",
   "metadata": {},
   "source": [
    "## Step 9: Batch Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: Batch Processing for Bad Channels & ICA\n",
    "def process_bad_channels_ica_batch(filtered_inventory_df, output_path, batch_size=20, max_files=None):\n",
    "    \"\"\"\n",
    "    Process all filtered files through bad channel detection and ICA.\n",
    "    \n",
    "    Args:\n",
    "        filtered_inventory_df: DataFrame of filtered files\n",
    "        output_path: Output directory path\n",
    "        batch_size: Number of files to process in each batch\n",
    "        max_files: Maximum number of files to process (None for all)\n",
    "    \"\"\"\n",
    "    print(\"üöÄ STARTING BATCH PROCESSING: BAD CHANNELS & ICA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Limit files if specified\n",
    "    files_to_process = filtered_inventory_df\n",
    "    if max_files and max_files < len(filtered_inventory_df):\n",
    "        files_to_process = filtered_inventory_df.head(max_files)\n",
    "    \n",
    "    total_files = len(files_to_process)\n",
    "    total_batches = (total_files + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"üìä Processing {total_files} files in {total_batches} batches\")\n",
    "    print(f\"üéØ Batch size: {batch_size}\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Create output directories\n",
    "    ica_cleaned_path = output_path / 'preprocessed_data' / 'ica_cleaned'\n",
    "    ica_cleaned_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    bad_channel_reports_path = output_path / 'preprocessed_data' / 'quality_reports'\n",
    "    bad_channel_reports_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        batch_start = batch_num * batch_size\n",
    "        batch_end = min((batch_num + 1) * batch_size, total_files)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"üîÑ BATCH {batch_num + 1}/{total_batches} (Files {batch_start + 1}-{batch_end})\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for idx in range(batch_start, batch_end):\n",
    "            row = files_to_process.iloc[idx]\n",
    "            file_num = idx + 1\n",
    "            \n",
    "            print(f\"   [{file_num}/{total_files}] {row['subject_id']} {row['session_id']}\")\n",
    "            \n",
    "            try:\n",
    "                # STEP 1: Load filtered data\n",
    "                raw_filtered = mne.io.read_raw_fif(row['file_path'], preload=True, verbose=False)\n",
    "                print(f\"      ‚úÖ Loaded: {len(raw_filtered.ch_names)} channels\")\n",
    "                \n",
    "                # STEP 2: Bad channel detection\n",
    "                bad_channels_results = detect_bad_channels_comprehensive(raw_filtered, method='auto')\n",
    "                \n",
    "                # Mark bad channels in the data\n",
    "                if bad_channels_results['bad_channels_identified']:\n",
    "                    raw_filtered.info['bads'] = bad_channels_results['bad_channels_identified']\n",
    "                    print(f\"      üî¥ Marked {len(bad_channels_results['bad_channels_identified'])} bad channels\")\n",
    "                \n",
    "                # STEP 3: ICA processing\n",
    "                ica, ica_results = perform_ica_processing(raw_filtered, n_components=0.95)\n",
    "                \n",
    "                # STEP 4: Save ICA-cleaned data\n",
    "                original_name = row['original_filename']\n",
    "                ica_filename = f\"{original_name}_ica_cleaned.fif\"\n",
    "                ica_filepath = ica_cleaned_path / ica_filename\n",
    "                \n",
    "                # Apply ICA (remove components automatically classified as artifacts)\n",
    "                artifact_components = identify_artifact_components(ica_results)\n",
    "                if artifact_components:\n",
    "                    print(f\"      üßπ Removing {len(artifact_components)} artifact components\")\n",
    "                    ica.apply(raw_filtered, exclude=artifact_components)\n",
    "                \n",
    "                # Save ICA-cleaned data\n",
    "                raw_filtered.save(ica_filepath, overwrite=True, verbose=False)\n",
    "                \n",
    "                # STEP 5: Save reports\n",
    "                # Bad channel report\n",
    "                bad_ch_report = {\n",
    "                    'subject_id': row['subject_id'],\n",
    "                    'session_id': row['session_id'],\n",
    "                    'task_type': row['task_type'],\n",
    "                    'original_filename': original_name,\n",
    "                    'bad_channels_detection': bad_channels_results,\n",
    "                    'ica_components_removed': artifact_components,\n",
    "                    'processing_timestamp': pd.Timestamp.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                bad_ch_report_path = bad_channel_reports_path / f\"{original_name}_badch_ica_report.json\"\n",
    "                with open(bad_ch_report_path, 'w') as f:\n",
    "                    json.dump(bad_ch_report, f, indent=2, default=str)\n",
    "                \n",
    "                # STEP 6: Create visualizations for first few files or periodically\n",
    "                if file_num <= 10 or file_num % 50 == 0:\n",
    "                    # Bad channel visualization\n",
    "                    create_bad_channel_visualization(raw_filtered, bad_channels_results, row, output_path)\n",
    "                    \n",
    "                    # ICA visualization\n",
    "                    create_ica_visualization(raw_filtered, ica, ica_results, row, output_path)\n",
    "                \n",
    "                processed_count += 1\n",
    "                print(f\"      ‚úÖ SUCCESS: {ica_filename}\")\n",
    "                \n",
    "                # Cleanup\n",
    "                del raw_filtered, ica\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"      ‚ùå ERROR: {str(e)[:80]}...\")\n",
    "                continue\n",
    "        \n",
    "        # Memory cleanup after each batch\n",
    "        gc.collect()\n",
    "        print(f\"   üßπ Memory cleared after batch {batch_num + 1}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üéØ BATCH PROCESSING COMPLETED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"üìä Results:\")\n",
    "    print(f\"   ‚Ä¢ Successfully processed: {processed_count}/{total_files}\")\n",
    "    print(f\"   ‚Ä¢ Errors: {error_count}\")\n",
    "    print(f\"   ‚Ä¢ ICA-cleaned files: {len(list(ica_cleaned_path.glob('*_ica_cleaned.fif')))}\")\n",
    "    \n",
    "    return processed_count, error_count\n",
    "\n",
    "def identify_artifact_components(ica_results):\n",
    "    \"\"\"Identify components to exclude based on automatic classification.\"\"\"\n",
    "    exclude_components = []\n",
    "    \n",
    "    for comp_key, comp_chars in ica_results['component_characteristics'].items():\n",
    "        # Exclude components classified as artifacts\n",
    "        if (comp_chars.get('is_eyeblink_likely') or \n",
    "            comp_chars.get('is_cardiac_likely') or \n",
    "            comp_chars.get('is_noise_likely')):\n",
    "            \n",
    "            comp_idx = int(comp_key.split('_')[1])\n",
    "            exclude_components.append(comp_idx)\n",
    "    \n",
    "    return exclude_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb966f",
   "metadata": {},
   "source": [
    "## Step 10: Run Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faaa170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Execute Batch Processing\n",
    "def execute_badchannel_ica_pipeline():\n",
    "    \"\"\"\n",
    "    Execute the complete bad channel and ICA pipeline.\n",
    "    \"\"\"\n",
    "    print(\"üéØ EXECUTING BAD CHANNEL & ICA PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    output_path = Path('EEG_Preprocessing_Output')\n",
    "    \n",
    "    # Check if filtered data exists\n",
    "    filtered_path = output_path / 'preprocessed_data' / 'raw_cleaned'\n",
    "    filtered_files = list(filtered_path.glob(\"*_filtered.fif\"))\n",
    "    \n",
    "    if not filtered_files:\n",
    "        print(\"‚ùå No filtered files found. Please run filtering pipeline first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÅ Found {len(filtered_files)} filtered files\")\n",
    "    \n",
    "    # Load filtered inventory\n",
    "    filtered_inventory_df = load_filtered_data_inventory(output_path)\n",
    "    \n",
    "    # Ask for processing parameters\n",
    "    print(\"\\n‚öôÔ∏è  PROCESSING PARAMETERS\")\n",
    "    max_files = input(\"Max files to process (Enter for all): \").strip()\n",
    "    max_files = int(max_files) if max_files else None\n",
    "    \n",
    "    batch_size = input(\"Batch size (Enter for 20): \").strip()\n",
    "    batch_size = int(batch_size) if batch_size else 20\n",
    "    \n",
    "    # Confirm processing\n",
    "    total_to_process = min(max_files, len(filtered_inventory_df)) if max_files else len(filtered_inventory_df)\n",
    "    response = input(f\"\\nProcess {total_to_process} files? (y/n): \")\n",
    "    \n",
    "    if response.lower() == 'y':\n",
    "        processed, errors = process_bad_channels_ica_batch(\n",
    "            filtered_inventory_df, output_path, \n",
    "            batch_size=batch_size, max_files=max_files\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüéâ PIPELINE COMPLETED!\")\n",
    "        print(f\"   ‚Ä¢ Processed: {processed} files\")\n",
    "        print(f\"   ‚Ä¢ Errors: {errors} files\")\n",
    "        print(f\"   ‚Ä¢ Output: preprocessed_data/ica_cleaned/\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Processing cancelled.\")\n",
    "\n",
    "# Execute the pipeline\n",
    "execute_badchannel_ica_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945bbaa8",
   "metadata": {},
   "source": [
    "## Step 11: Pipeline Progress & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faaf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 11: Comprehensive Progress Check\n",
    "def check_badchannel_ica_progress(output_path):\n",
    "    \"\"\"\n",
    "    Check progress of bad channel detection and ICA pipeline.\n",
    "    \n",
    "    Args:\n",
    "        output_path: Main output directory path\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä BAD CHANNEL & ICA PIPELINE PROGRESS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Count files in each stage\n",
    "    raw_cleaned_path = output_path / 'preprocessed_data' / 'raw_cleaned'\n",
    "    ica_cleaned_path = output_path / 'preprocessed_data' / 'ica_cleaned'\n",
    "    quality_reports_path = output_path / 'preprocessed_data' / 'quality_reports'\n",
    "    visualizations_path = output_path / 'preprocessed_data' / 'visualizations'\n",
    "    \n",
    "    # File counts\n",
    "    filtered_files = list(raw_cleaned_path.glob(\"*_filtered.fif\"))\n",
    "    ica_cleaned_files = list(ica_cleaned_path.glob(\"*_ica_cleaned.fif\"))\n",
    "    badchannel_reports = list(quality_reports_path.glob(\"*_badch_ica_report.json\"))\n",
    "    badchannel_viz = list(visualizations_path.glob(\"*_bad_channels.png\"))\n",
    "    ica_viz = list(visualizations_path.glob(\"*_ica_components.png\"))\n",
    "    \n",
    "    total_filtered = len(filtered_files)\n",
    "    \n",
    "    print(f\"üìà PIPELINE PROGRESS STATISTICS:\")\n",
    "    print(f\"   ‚Ä¢ Filtered files available: {total_filtered}\")\n",
    "    print(f\"   ‚Ä¢ ICA-cleaned files: {len(ica_cleaned_files)}\")\n",
    "    print(f\"   ‚Ä¢ Bad channel/ICA reports: {len(badchannel_reports)}\")\n",
    "    print(f\"   ‚Ä¢ Bad channel visualizations: {len(badchannel_viz)}\")\n",
    "    print(f\"   ‚Ä¢ ICA component visualizations: {len(ica_viz)}\")\n",
    "    \n",
    "    # Completion percentages\n",
    "    if total_filtered > 0:\n",
    "        ica_pct = (len(ica_cleaned_files) / total_filtered) * 100\n",
    "        report_pct = (len(badchannel_reports) / total_filtered) * 100\n",
    "        \n",
    "        print(f\"\\nüéØ COMPLETION STATUS:\")\n",
    "        print(f\"   ‚Ä¢ ICA Processing: {ica_pct:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Quality Reports: {report_pct:.1f}%\")\n",
    "    \n",
    "    # Show sample files\n",
    "    if ica_cleaned_files:\n",
    "        print(f\"\\nüìù Sample ICA-cleaned files:\")\n",
    "        for f in ica_cleaned_files[:3]:\n",
    "            file_size = f.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   ‚Ä¢ {f.name} ({file_size:.1f} MB)\")\n",
    "    \n",
    "    # Recommendations\n",
    "    if len(ica_cleaned_files) >= total_filtered * 0.9:\n",
    "        print(f\"\\n   ‚úÖ READY: Bad channel & ICA pipeline complete!\")\n",
    "        print(f\"   ‚Üí Proceed to Epoching & Feature Extraction\")\n",
    "    elif len(ica_cleaned_files) == 0:\n",
    "        print(f\"\\n   üîÑ NEED: Run bad channel & ICA pipeline\")\n",
    "        print(f\"   ‚Üí Execute batch processing above\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚ö†Ô∏è  PARTIAL: Pipeline incomplete\")\n",
    "        print(f\"   ‚Üí Continue batch processing or check for errors\")\n",
    "    \n",
    "    return {\n",
    "        'filtered_files': total_filtered,\n",
    "        'ica_cleaned_files': len(ica_cleaned_files),\n",
    "        'badchannel_reports': len(badchannel_reports),\n",
    "        'badchannel_viz': len(badchannel_viz),\n",
    "        'ica_viz': len(ica_viz)\n",
    "    }\n",
    "\n",
    "# Check progress\n",
    "pipeline_progress = check_badchannel_ica_progress(Path('EEG_Preprocessing_Output'))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Summary\n",
    "# \n",
    "# This notebook completes the third phase of EEG preprocessing with:\n",
    "# \n",
    "# ‚úÖ **Bad Channel Detection**: Automated identification of problematic channels using statistical methods  \n",
    "# ‚úÖ **ICA Processing**: Artifact removal using Independent Component Analysis  \n",
    "# ‚úÖ **Quality Reports**: Comprehensive reporting for each processing step  \n",
    "# ‚úÖ **Visualizations**: Professional plots for quality assessment  \n",
    "# ‚úÖ **Batch Processing**: Efficient processing of all files  \n",
    "# \n",
    "# **Next Steps**: Proceed to epoching and feature extraction for analysis-ready data.\n",
    "\n",
    "print(\"\\nüéâ Bad Channel Detection & ICA Pipeline Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0714139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imenv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
