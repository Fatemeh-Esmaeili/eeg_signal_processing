{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd61fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING PROPER DATA ORGANIZATION ===\n",
      "Base directory: D:\\impress_project\\eeg_signals\\data\\LRMI-21679035\n",
      "Organized directory: D:\\impress_project\\eeg_signals\\data\\LRMI-21679035\\organized_data_v2\n",
      "âœ“ Created main directory: D:\\impress_project\\eeg_signals\\data\\LRMI-21679035\\organized_data_v2\n",
      "  Created: raw_data\n",
      "  Created: paper_preprocessed\n",
      "  Created: my_preprocessed\n",
      "  Created: patient_info\n",
      "  Created: events\n",
      "  Created: results\n",
      "  Created: plots\n",
      "  Created: code\n",
      "  Created: temp\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create proper data organization structure\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CREATING PROPER DATA ORGANIZATION ===\")\n",
    "\n",
    "# Define paths\n",
    "base_dir = r'D:\\impress_project\\eeg_signals\\data\\LRMI-21679035'\n",
    "organized_dir = os.path.join(base_dir, 'organized_data_v2')\n",
    "\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "print(f\"Organized directory: {organized_dir}\")\n",
    "\n",
    "# Create organized directory structure\n",
    "folders_to_create = [\n",
    "    'raw_data',              # Original .mat files\n",
    "    'paper_preprocessed',    # Paper's preprocessed .edf files\n",
    "    'my_preprocessed',       # Your preprocessing results\n",
    "    'patient_info',          # Patient metadata\n",
    "    'events',                # Event markers\n",
    "    'results',               # Analysis results\n",
    "    'plots',                 # Visualizations\n",
    "    'code',                  # Analysis scripts\n",
    "    'temp'                   # Temporary files\n",
    "]\n",
    "\n",
    "# Create main organized directory\n",
    "if not os.path.exists(organized_dir):\n",
    "    os.makedirs(organized_dir)\n",
    "    print(f\"âœ“ Created main directory: {organized_dir}\")\n",
    "else:\n",
    "    print(f\"âœ“ Directory already exists: {organized_dir}\")\n",
    "\n",
    "# Create subdirectories\n",
    "for folder in folders_to_create:\n",
    "    folder_path = os.path.join(organized_dir, folder)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"  Created: {folder}\")\n",
    "    else:\n",
    "        print(f\"  Exists: {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be250098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 1: ORGANIZING RAW DATA (.mat files) ===\n",
      "Found 50 raw .mat files\n",
      "  Copied 10/50: sub-10\n",
      "  Copied 20/50: sub-20\n",
      "  Copied 30/50: sub-30\n",
      "  Copied 40/50: sub-40\n",
      "  Copied 50/50: sub-50\n",
      "\n",
      "âœ… Successfully copied: 50 raw files\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Organize RAW Data (.mat files)\n",
    "\n",
    "print(\"\\n=== STEP 2: ORGANIZING RAW DATA (.mat files) ===\")\n",
    "\n",
    "# Find all raw .mat files\n",
    "raw_source_dir = os.path.join(base_dir, 'sourcedata')\n",
    "raw_files = []\n",
    "\n",
    "if os.path.exists(raw_source_dir):\n",
    "    for root, dirs, files in os.walk(raw_source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.mat') and 'task-motor-imagery' in file:\n",
    "                full_path = os.path.join(root, file)\n",
    "                raw_files.append(full_path)\n",
    "    \n",
    "    print(f\"Found {len(raw_files)} raw .mat files\")\n",
    "    \n",
    "    # Copy raw files\n",
    "    raw_dest_dir = os.path.join(organized_dir, 'raw_data')\n",
    "    raw_files_copied = []\n",
    "    \n",
    "    for i, source_file in enumerate(raw_files, 1):\n",
    "        try:\n",
    "            # Extract subject ID from file path\n",
    "            # Path format: .../sourcedata/sub-01/sub-01_task-motor-imagery_eeg.mat\n",
    "            subject_folder = os.path.basename(os.path.dirname(source_file))\n",
    "            file_name = os.path.basename(source_file)\n",
    "            \n",
    "            # Create target filename\n",
    "            target_file = os.path.join(raw_dest_dir, file_name)\n",
    "            \n",
    "            # Copy the file\n",
    "            shutil.copy2(source_file, target_file)\n",
    "            raw_files_copied.append((subject_folder, file_name))\n",
    "            \n",
    "            if i % 10 == 0 or i == len(raw_files):\n",
    "                print(f\"  Copied {i}/{len(raw_files)}: {subject_folder}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Failed to copy {source_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Successfully copied: {len(raw_files_copied)} raw files\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âœ— Raw data directory not found: {raw_source_dir}\")\n",
    "    raw_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2590d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 2: ORGANIZING PAPER'S PREPROCESSED DATA (.edf files) ===\n",
      "Found 50 preprocessed .edf files\n",
      "  Copied 10/50: sub-10\n",
      "  Copied 20/50: sub-20\n",
      "  Copied 30/50: sub-30\n",
      "  Copied 40/50: sub-40\n",
      "  Copied 50/50: sub-50\n",
      "\n",
      "âœ… Successfully copied: 50 preprocessed files\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Organize Paper's Preprocessed Data (.edf files)\n",
    "\n",
    "print(\"\\n=== STEP 3: ORGANIZING PAPER'S PREPROCESSED DATA (.edf files) ===\")\n",
    "\n",
    "# Find all preprocessed .edf files\n",
    "preprocessed_source_dir = os.path.join(base_dir, 'edffile')\n",
    "preprocessed_files = []\n",
    "\n",
    "if os.path.exists(preprocessed_source_dir):\n",
    "    for root, dirs, files in os.walk(preprocessed_source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.edf'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                preprocessed_files.append(full_path)\n",
    "    \n",
    "    print(f\"Found {len(preprocessed_files)} preprocessed .edf files\")\n",
    "    \n",
    "    # Copy preprocessed files\n",
    "    preprocessed_dest_dir = os.path.join(organized_dir, 'paper_preprocessed')\n",
    "    preprocessed_files_copied = []\n",
    "    \n",
    "    for i, source_file in enumerate(preprocessed_files, 1):\n",
    "        try:\n",
    "            # Extract subject ID from file path\n",
    "            # Path format: .../edffile/sub-01/eeg/sub-01_task-motor-imagery_eeg.edf\n",
    "            # Go up two levels to get subject folder\n",
    "            subject_folder = os.path.basename(os.path.dirname(os.path.dirname(source_file)))\n",
    "            file_name = os.path.basename(source_file)\n",
    "            \n",
    "            # Create new filename (consistent with raw data naming)\n",
    "            new_file_name = file_name  # Keep original name\n",
    "            target_file = os.path.join(preprocessed_dest_dir, new_file_name)\n",
    "            \n",
    "            # Copy the file\n",
    "            shutil.copy2(source_file, target_file)\n",
    "            preprocessed_files_copied.append((subject_folder, new_file_name))\n",
    "            \n",
    "            if i % 10 == 0 or i == len(preprocessed_files):\n",
    "                print(f\"  Copied {i}/{len(preprocessed_files)}: {subject_folder}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Failed to copy {source_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Successfully copied: {len(preprocessed_files_copied)} preprocessed files\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âœ— Preprocessed data directory not found: {preprocessed_source_dir}\")\n",
    "    preprocessed_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caafe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 3: CREATING MASTER FILE INDEX ===\n",
      "âœ“ Master index saved to: D:\\impress_project\\eeg_signals\\data\\LRMI-21679035\\organized_data_v2\\data_files_index.csv\n",
      "\n",
      "ðŸ“Š Dataset Summary:\n",
      "  Total unique subjects: 50\n",
      "  Raw files (.mat): 50\n",
      "  Preprocessed files (.edf): 50\n",
      "\n",
      "ðŸ“‹ File Availability per Subject:\n",
      "subject_id                 data_type  has_raw  has_preprocessed\n",
      "    sub-01 [paper_preprocessed, raw]     True              True\n",
      "    sub-02 [paper_preprocessed, raw]     True              True\n",
      "    sub-03 [paper_preprocessed, raw]     True              True\n",
      "    sub-04 [paper_preprocessed, raw]     True              True\n",
      "    sub-05 [paper_preprocessed, raw]     True              True\n",
      "    sub-06 [paper_preprocessed, raw]     True              True\n",
      "    sub-07 [paper_preprocessed, raw]     True              True\n",
      "    sub-08 [paper_preprocessed, raw]     True              True\n",
      "    sub-09 [paper_preprocessed, raw]     True              True\n",
      "    sub-10 [paper_preprocessed, raw]     True              True\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create a Master Index File\n",
    "\n",
    "\n",
    "print(\"\\n=== STEP 4: CREATING MASTER FILE INDEX ===\")\n",
    "\n",
    "# Create a comprehensive index of all files\n",
    "file_info = []\n",
    "\n",
    "# Get all subject IDs\n",
    "all_subject_ids = set()\n",
    "\n",
    "# Add raw files to index\n",
    "raw_dest_dir = os.path.join(organized_dir, 'raw_data')\n",
    "if os.path.exists(raw_dest_dir):\n",
    "    raw_files_list = os.listdir(raw_dest_dir)\n",
    "    for file in raw_files_list:\n",
    "        if file.endswith('.mat'):\n",
    "            # Extract subject ID from filename: sub-01_task-motor-imagery_eeg.mat\n",
    "            subject_id = file.split('_')[0]\n",
    "            all_subject_ids.add(subject_id)\n",
    "            \n",
    "            file_info.append({\n",
    "                'subject_id': subject_id,\n",
    "                'data_type': 'raw',\n",
    "                'format': 'mat',\n",
    "                'file_name': file,\n",
    "                'file_path': os.path.join(raw_dest_dir, file),\n",
    "                'source': 'sourcedata',\n",
    "                'description': 'Raw EEG data with labels'\n",
    "            })\n",
    "\n",
    "# Add paper preprocessed files to index\n",
    "preprocessed_dest_dir = os.path.join(organized_dir, 'paper_preprocessed')\n",
    "if os.path.exists(preprocessed_dest_dir):\n",
    "    preprocessed_files_list = os.listdir(preprocessed_dest_dir)\n",
    "    for file in preprocessed_files_list:\n",
    "        if file.endswith('.edf'):\n",
    "            # Extract subject ID from filename: sub-01_task-motor-imagery_eeg.edf\n",
    "            subject_id = file.split('_')[0]\n",
    "            all_subject_ids.add(subject_id)\n",
    "            \n",
    "            file_info.append({\n",
    "                'subject_id': subject_id,\n",
    "                'data_type': 'paper_preprocessed',\n",
    "                'format': 'edf',\n",
    "                'file_name': file,\n",
    "                'file_path': os.path.join(preprocessed_dest_dir, file),\n",
    "                'source': 'edffile',\n",
    "                'description': 'Paper preprocessed data (0.5-40 Hz filtered)'\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "files_df = pd.DataFrame(file_info)\n",
    "\n",
    "# Sort by subject ID and data type\n",
    "files_df['subject_num'] = files_df['subject_id'].apply(lambda x: int(x.split('-')[1]))\n",
    "files_df = files_df.sort_values(['subject_num', 'data_type'])\n",
    "\n",
    "# Save to CSV\n",
    "index_file = os.path.join(organized_dir, 'data_files_index.csv')\n",
    "files_df.to_csv(index_file, index=False)\n",
    "print(f\"âœ“ Master index saved to: {index_file}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(f\"  Total unique subjects: {len(all_subject_ids)}\")\n",
    "print(f\"  Raw files (.mat): {len(files_df[files_df['data_type'] == 'raw'])}\")\n",
    "print(f\"  Preprocessed files (.edf): {len(files_df[files_df['data_type'] == 'paper_preprocessed'])}\")\n",
    "\n",
    "# Check for missing files\n",
    "print(\"\\nðŸ“‹ File Availability per Subject:\")\n",
    "subject_summary = files_df.groupby('subject_id')['data_type'].apply(list).reset_index()\n",
    "subject_summary['has_raw'] = subject_summary['data_type'].apply(lambda x: 'raw' in x)\n",
    "subject_summary['has_preprocessed'] = subject_summary['data_type'].apply(lambda x: 'paper_preprocessed' in x)\n",
    "\n",
    "print(subject_summary.head(10).to_string(index=False))\n",
    "\n",
    "missing_raw = subject_summary[~subject_summary['has_raw']]['subject_id'].tolist()\n",
    "missing_preprocessed = subject_summary[~subject_summary['has_preprocessed']]['subject_id'].tolist()\n",
    "\n",
    "if missing_raw:\n",
    "    print(f\"\\nâš  Missing raw data for subjects: {missing_raw}\")\n",
    "if missing_preprocessed:\n",
    "    print(f\"âš  Missing preprocessed data for subjects: {missing_preprocessed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 4: COPYING PATIENT INFORMATION ===\n",
      "âœ“ Copied participants.tsv to: D:\\impress_project\\eeg_signals\\data\\LRMI-21679035\\organized_data_v2\\patient_info\\participants.tsv\n",
      "  Contains 50 patients\n",
      "\n",
      "Copying BIDS metadata files...\n",
      "âœ“ Copied dataset_description.json\n",
      "âœ“ Copied participants.json\n",
      "âœ“ Copied task-motor-imagery_eeg.json\n",
      "âœ“ Copied task-motor-imagery_events.json\n",
      "âœ“ Copied task-motor-imagery_channels.tsv\n",
      "âœ“ Copied task-motor-imagery_electrodes.tsv\n",
      "âœ“ Copied task-motor-imagery_coordsystem.json\n",
      "âœ“ Copied README.md\n",
      "\n",
      "Copying event files...\n",
      "âœ“ Copied event markers to: D:\\impress_project\\eeg_signals\\data\\LRMI-21679035\\organized_data_v2\\events\\task-motor-imagery_events.tsv\n",
      "  Events file contains 120 markers\n",
      "  Event types: [1 2]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Copy Patient Information and Metadata\n",
    "print(\"\\n=== STEP 5: COPYING PATIENT INFORMATION ===\")\n",
    "\n",
    "# Copy participants.tsv\n",
    "participants_source = os.path.join(base_dir, 'participants.tsv')\n",
    "participants_target = os.path.join(organized_dir, 'patient_info', 'participants.tsv')\n",
    "\n",
    "if os.path.exists(participants_source):\n",
    "    shutil.copy2(participants_source, participants_target)\n",
    "    print(f\"âœ“ Copied participants.tsv to: {participants_target}\")\n",
    "    \n",
    "    # Load and display summary\n",
    "    participants_df = pd.read_csv(participants_target, sep='\\t')\n",
    "    print(f\"  Contains {len(participants_df)} patients\")\n",
    "else:\n",
    "    print(f\"âœ— participants.tsv not found at: {participants_source}\")\n",
    "\n",
    "# Copy all BIDS metadata files\n",
    "print(\"\\nCopying BIDS metadata files...\")\n",
    "bids_files_to_copy = [\n",
    "    'dataset_description.json',\n",
    "    'participants.json',\n",
    "    'task-motor-imagery_eeg.json',\n",
    "    'task-motor-imagery_events.json',\n",
    "    'task-motor-imagery_channels.tsv',\n",
    "    'task-motor-imagery_electrodes.tsv',\n",
    "    'task-motor-imagery_coordsystem.json',\n",
    "    'README.md'\n",
    "]\n",
    "\n",
    "for file_name in bids_files_to_copy:\n",
    "    source_path = os.path.join(base_dir, file_name)\n",
    "    target_path = os.path.join(organized_dir, 'patient_info', file_name)\n",
    "    \n",
    "    if os.path.exists(source_path):\n",
    "        shutil.copy2(source_path, target_path)\n",
    "        print(f\"âœ“ Copied {file_name}\")\n",
    "    else:\n",
    "        print(f\"âš  {file_name} not found (optional file)\")\n",
    "\n",
    "# Copy event files separately\n",
    "print(\"\\nCopying event files...\")\n",
    "events_source = os.path.join(base_dir, 'task-motor-imagery_events.tsv')\n",
    "events_target = os.path.join(organized_dir, 'events', 'task-motor-imagery_events.tsv')\n",
    "\n",
    "if os.path.exists(events_source):\n",
    "    shutil.copy2(events_source, events_target)\n",
    "    print(f\"âœ“ Copied event markers to: {events_target}\")\n",
    "    \n",
    "    # Load events to check structure\n",
    "    events_df = pd.read_csv(events_target, sep='\\t')\n",
    "    print(f\"  Events file contains {len(events_df)} markers\")\n",
    "    if 'trial_type' in events_df.columns:\n",
    "        print(f\"  Event types: {events_df['trial_type'].unique()}\")\n",
    "else:\n",
    "    print(f\"âœ— Event markers not found at: {events_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb9144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 5: CREATING ENHANCED CONFIGURATION ===\n",
      "âœ“ Configuration file created at: D:\\impress_project\\eeg_signals\\data\\LRMI-21679035\\organized_data_v2\\config.json\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create Enhanced Configuration File\n",
    "\n",
    "print(\"\\n=== STEP 6: CREATING ENHANCED CONFIGURATION ===\")\n",
    "\n",
    "# Create detailed configuration\n",
    "config_content = {\n",
    "    \"dataset\": {\n",
    "        \"name\": \"EEG Motor Imagery for Acute Stroke Patients\",\n",
    "        \"source_paper\": \"An EEG motor imagery dataset for brain computer interface in acute stroke patients\",\n",
    "        \"n_subjects_total\": 50,\n",
    "        \"n_subjects_available\": len(all_subject_ids),\n",
    "        \"data_types\": {\n",
    "            \"raw\": {\n",
    "                \"format\": \"mat\",\n",
    "                \"description\": \"Original raw data with labels\",\n",
    "                \"sampling_rate\": 500,\n",
    "                \"contents\": \"rawdata (trialsÃ—channelsÃ—samples), labels (1=left, 2=right)\",\n",
    "                \"preprocessing\": \"None\"\n",
    "            },\n",
    "            \"paper_preprocessed\": {\n",
    "                \"format\": \"edf\",\n",
    "                \"description\": \"Paper's preprocessed data\",\n",
    "                \"sampling_rate\": 500,\n",
    "                \"preprocessing\": \"0.5-40 Hz bandpass filter, baseline correction\",\n",
    "                \"reference\": \"CPz\"\n",
    "            },\n",
    "            \"my_preprocessed\": {\n",
    "                \"format\": \"To be determined\",\n",
    "                \"description\": \"Your custom preprocessing results\",\n",
    "                \"location\": \"my_preprocessed folder\"\n",
    "            }\n",
    "        },\n",
    "        \"trial_structure\": {\n",
    "            \"trials_per_subject\": 40,\n",
    "            \"left_hand_trials\": 20,\n",
    "            \"right_hand_trials\": 20,\n",
    "            \"trial_duration\": 8,\n",
    "            \"instruction_period\": 2,\n",
    "            \"motor_imagery_period\": 4,\n",
    "            \"break_period\": 2\n",
    "        }\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"organized_root\": organized_dir,\n",
    "        \"raw_data\": os.path.join(organized_dir, \"raw_data\"),\n",
    "        \"paper_preprocessed\": os.path.join(organized_dir, \"paper_preprocessed\"),\n",
    "        \"my_preprocessed\": os.path.join(organized_dir, \"my_preprocessed\"),\n",
    "        \"patient_info\": os.path.join(organized_dir, \"patient_info\"),\n",
    "        \"events\": os.path.join(organized_dir, \"events\"),\n",
    "        \"results\": os.path.join(organized_dir, \"results\"),\n",
    "        \"plots\": os.path.join(organized_dir, \"plots\"),\n",
    "        \"code\": os.path.join(organized_dir, \"code\")\n",
    "    },\n",
    "    \"eeg_specifications\": {\n",
    "        \"system\": \"ZhenTec NT1 wireless portable EEG\",\n",
    "        \"montage\": \"International 10-10 system\",\n",
    "        \"total_channels\": 33,\n",
    "        \"eeg_channels\": 30,\n",
    "        \"eog_channels\": 2,\n",
    "        \"marker_channel\": 1,\n",
    "        \"reference\": \"CPz\",\n",
    "        \"ground\": \"FPz\",\n",
    "        \"sampling_rate\": 500,\n",
    "        \"filter_applied\": \"0.5-40 Hz (paper preprocessed)\"\n",
    "    },\n",
    "    \"important_channels\": {\n",
    "        \"motor_cortex\": [\"C3\", \"C4\", \"Cz\"],\n",
    "        \"motor_adjacent\": [\"FC3\", \"FC4\", \"CP3\", \"CP4\"],\n",
    "        \"eog_channels\": [\"HEOL\", \"VEOR\"],\n",
    "        \"reference\": \"CPz\",\n",
    "        \"ground\": \"FPz\"\n",
    "    },\n",
    "    \"analysis_parameters\": {\n",
    "        \"mi_analysis_band\": [8, 30],  # Hz, for motor imagery analysis\n",
    "        \"baseline_period\": [-1, 0],  # seconds relative to MI onset\n",
    "        \"mi_period\": [0, 4],  # seconds of motor imagery\n",
    "        \"erds_window\": 0.5,  # seconds for ERD/S calculation\n",
    "        \"expected_accuracy\": \"72.21% (paper's best method)\"\n",
    "    },\n",
    "    \"file_index\": {\n",
    "        \"location\": os.path.join(organized_dir, \"data_files_index.csv\"),\n",
    "        \"total_files\": len(files_df),\n",
    "        \"raw_files\": len(files_df[files_df['data_type'] == 'raw']),\n",
    "        \"preprocessed_files\": len(files_df[files_df['data_type'] == 'paper_preprocessed'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "config_path = os.path.join(organized_dir, 'config.json')\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_content, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ“ Configuration file created at: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548f954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING README FILE ===\n",
      "âœ“ README created at: D:\\impress_project\\eeg_signals\\data\\LRMI-21679035\\organized_data_v2\\README.md\n",
      "  - Raw data files: 50\n",
      "  - Preprocessed files: 50\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Create README File with Dataset Information\n",
    "\n",
    "print(\"\\n=== Step 7:CREATING README FILE ===\")\n",
    "\n",
    "# Get counts from the actual organized structure\n",
    "raw_data_count = len([f for f in os.listdir(os.path.join(organized_dir, 'raw_data')) \n",
    "                     if f.endswith('.mat')]) if os.path.exists(os.path.join(organized_dir, 'raw_data')) else 0\n",
    "\n",
    "paper_preprocessed_count = len([f for f in os.listdir(os.path.join(organized_dir, 'paper_preprocessed')) \n",
    "                               if f.endswith('.edf')]) if os.path.exists(os.path.join(organized_dir, 'paper_preprocessed')) else 0\n",
    "\n",
    "# Load participants data if available\n",
    "participants_path = os.path.join(organized_dir, 'patient_info', 'participants.tsv')\n",
    "if os.path.exists(participants_path):\n",
    "    participants_df = pd.read_csv(participants_path, sep='\\t')\n",
    "    age_range = f\"{participants_df['Age'].min()} - {participants_df['Age'].max()}\"\n",
    "    gender_dist = dict(participants_df['Gender'].value_counts())\n",
    "else:\n",
    "    participants_df = None\n",
    "    age_range = \"Unknown\"\n",
    "    gender_dist = \"Unknown\"\n",
    "\n",
    "# Create a README with the UPDATED dataset information\n",
    "readme_content = f\"\"\"\n",
    "# EEG Motor Imagery Dataset - Organized Structure\n",
    "\n",
    "## Dataset Information\n",
    "- Source: \"An EEG motor imagery dataset for brain computer interface in acute stroke patients\"\n",
    "- Total subjects: {len(files_df) if not files_df.empty else '50'}\n",
    "- Data formats: .mat (raw) and .edf (preprocessed)\n",
    "- Sampling rate: 500 Hz\n",
    "- Channels: 30 EEG + 2 EOG + 1 marker (based on paper)\n",
    "- Trials per subject: 40 (20 left hand, 20 right hand MI)\n",
    "\n",
    "## Folder Structure\n",
    "organized_data/\n",
    "â”œâ”€â”€ raw_data/                    # Original raw .mat files\n",
    "â”‚   â”œâ”€â”€ sub-01_task-motor-imagery_eeg.mat\n",
    "â”‚   â”œâ”€â”€ sub-02_task-motor-imagery_eeg.mat\n",
    "â”‚   â””â”€â”€ ... ({raw_data_count} files total)\n",
    "â”œâ”€â”€ paper_preprocessed/          # Paper's preprocessed .edf files\n",
    "â”‚   â”œâ”€â”€ sub-01_task-motor-imagery_eeg.edf\n",
    "â”‚   â”œâ”€â”€ sub-02_task-motor-imagery_eeg.edf\n",
    "â”‚   â””â”€â”€ ... ({paper_preprocessed_count} files total)\n",
    "â”œâ”€â”€ patient_info/               # Patient metadata and demographics\n",
    "â”‚   â”œâ”€â”€ participants.tsv\n",
    "â”‚   â”œâ”€â”€ participants.json\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ events/                     # Event markers and triggers\n",
    "â”‚   â”œâ”€â”€ task-motor-imagery_events.tsv\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ my_preprocessed/           # Your preprocessing results (empty)\n",
    "â”œâ”€â”€ results/                   # Analysis results\n",
    "â”œâ”€â”€ plots/                     # Visualizations\n",
    "â”œâ”€â”€ code/                      # Analysis scripts\n",
    "â””â”€â”€ temp/                      # Temporary files\n",
    "\n",
    "## File Naming Convention\n",
    "- Raw files: [subject_id]_task-motor-imagery_eeg.mat\n",
    "- Preprocessed files: [subject_id]_task-motor-imagery_eeg.edf\n",
    "- Example: sub-01_task-motor-imagery_eeg.mat (raw)\n",
    "- Example: sub-01_task-motor-imagery_eeg.edf (preprocessed)\n",
    "\n",
    "## Patient Information\n",
    "- Total: {len(participants_df) if participants_df is not None else 'Unknown'}\n",
    "- Age range: {age_range} years\n",
    "- Gender: {gender_dist}\n",
    "\n",
    "## Analysis Notes\n",
    "1. All EEG files have been verified to contain C3 and C4 channels\n",
    "2. Paper's preprocessed data: 0.5-40 Hz filtered\n",
    "3. For MI analysis, apply additional 8-30 Hz filter as per paper\n",
    "4. Event markers should be used to segment 4-second MI periods\n",
    "5. Raw data (.mat) contains trial labels: 1=left hand MI, 2=right hand MI\n",
    "\n",
    "## Created on\n",
    "{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "readme_path = os.path.join(organized_dir, 'README.md')\n",
    "with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"âœ“ README created at: {readme_path}\")\n",
    "print(f\"  - Raw data files: {raw_data_count}\")\n",
    "print(f\"  - Preprocessed files: {paper_preprocessed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa255d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VERIFYING ORGANIZED STRUCTURE ===\n",
      "\n",
      "File counts in organized structure:\n",
      "  raw_data: 50 files\n",
      "  paper_preprocessed: 50 files\n",
      "  my_preprocessed: 0 files\n",
      "  patient_info: 9 files\n",
      "  events: 1 files\n",
      "  results: 0 files\n",
      "  plots: 0 files\n",
      "  code: 1 files\n",
      "  temp: 0 files\n",
      "\n",
      "Verifying EEG file accessibility...\n",
      "\n",
      "âœ… Successfully verified 0 EEG files\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Verify the Organized Structure\n",
    "\n",
    "print(\"\\n=== Step 8: VERIFYING ORGANIZED STRUCTURE ===\")\n",
    "\n",
    "# Count files in each directory\n",
    "print(\"\\nFile counts in organized structure:\")\n",
    "for folder in folders_to_create:\n",
    "    folder_path = os.path.join(organized_dir, folder)\n",
    "    if os.path.exists(folder_path):\n",
    "        files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        print(f\"  {folder}: {len(files)} files\")\n",
    "\n",
    "# Verify EEG files are accessible\n",
    "print(\"\\nVerifying EEG file accessibility...\")\n",
    "accessible_files = 0\n",
    "eeg_folder = os.path.join(organized_dir, 'eeg_data')\n",
    "\n",
    "if os.path.exists(eeg_folder):\n",
    "    for file in os.listdir(eeg_folder)[:3]:  # Check first 3 files\n",
    "        file_path = os.path.join(eeg_folder, file)\n",
    "        try:\n",
    "            # Try to load file info (not full data, for speed)\n",
    "            raw_test = mne.io.read_raw_edf(file_path, preload=False, verbose=False)\n",
    "            accessible_files += 1\n",
    "            print(f\"  âœ“ {file}: {len(raw_test.ch_names)} channels, {raw_test.info['sfreq']} Hz\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— {file}: Error - {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully verified {accessible_files} EEG files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997139ad",
   "metadata": {},
   "source": [
    "The data loader does not work properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ca899",
   "metadata": {},
   "source": [
    "# Step 9: Create Data Loader Script for Both Data Types\n",
    "\n",
    "\n",
    "It does not still work properly\n",
    "\n",
    "print(\"\\n=== STEP 9: CREATING DATA LOADER SCRIPT ===\")\n",
    "\n",
    "loader_script_content = '''\"\"\"\n",
    "Data Loader for EEG Motor Imagery Dataset\n",
    "Supports both raw (.mat) and preprocessed (.edf) data\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "\n",
    "class EEGDatasetLoader:\n",
    "    def __init__(self, base_path=None):\n",
    "        \"\"\"Initialize dataset loader\"\"\"\n",
    "        if base_path is None:\n",
    "            # Try to find the organized_data folder\n",
    "            current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            base_path = os.path.join(current_dir, '..', 'organized_data_v2')\n",
    "        \n",
    "        self.base_path = base_path\n",
    "        \n",
    "        # Load configuration\n",
    "        config_path = os.path.join(base_path, 'config.json')\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Load file index\n",
    "        index_path = os.path.join(base_path, 'data_files_index.csv')\n",
    "        self.file_index = pd.read_csv(index_path)\n",
    "        \n",
    "        # Load patient info\n",
    "        participants_path = os.path.join(base_path, 'patient_info', 'participants.tsv')\n",
    "        self.participants = pd.read_csv(participants_path, sep='\\\\t')\n",
    "        \n",
    "        # Load events if available\n",
    "        events_path = os.path.join(base_path, 'events', 'task-motor-imagery_events.tsv')\n",
    "        if os.path.exists(events_path):\n",
    "            self.events = pd.read_csv(events_path, sep='\\\\t')\n",
    "        else:\n",
    "            self.events = None\n",
    "        \n",
    "        print(f\"Dataset loaded: {self.config['dataset']['name']}\")\n",
    "        print(f\"Subjects: {self.get_n_subjects()}\")\n",
    "        print(f\"Data types available: {self.get_available_data_types()}\")\n",
    "    \n",
    "    def get_n_subjects(self):\n",
    "        \"\"\"Get number of unique subjects\"\"\"\n",
    "        return len(self.file_index['subject_id'].unique())\n",
    "    \n",
    "    def get_available_data_types(self):\n",
    "        \"\"\"Get available data types\"\"\"\n",
    "        return self.file_index['data_type'].unique().tolist()\n",
    "    \n",
    "    def get_subject_files(self, subject_id, data_type=None):\n",
    "        \"\"\"Get file information for a subject\"\"\"\n",
    "        if data_type:\n",
    "            files = self.file_index[\n",
    "                (self.file_index['subject_id'] == subject_id) & \n",
    "                (self.file_index['data_type'] == data_type)\n",
    "            ]\n",
    "        else:\n",
    "            files = self.file_index[self.file_index['subject_id'] == subject_id]\n",
    "        \n",
    "        return files\n",
    "    \n",
    "    def load_raw_mat_data(self, subject_id):\n",
    "        \"\"\"Load raw .mat data for a subject\"\"\"\n",
    "        files = self.get_subject_files(subject_id, 'raw')\n",
    "        \n",
    "        if len(files) == 0:\n",
    "            raise ValueError(f\"No raw data found for subject {subject_id}\")\n",
    "        \n",
    "        file_path = files.iloc[0]['file_path']\n",
    "        \n",
    "        print(f\"Loading raw .mat data: {subject_id}\")\n",
    "        mat_data = loadmat(file_path)\n",
    "        \n",
    "        # Extract data based on paper description\n",
    "        # According to paper: 'mat' file contains 'rawdata' and 'labels'\n",
    "        if 'rawdata' in mat_data:\n",
    "            rawdata = mat_data['rawdata']  # trials Ã— channels Ã— time samples\n",
    "            labels = mat_data['labels'].flatten() if 'labels' in mat_data else None\n",
    "            \n",
    "            # Get dimensions\n",
    "            n_trials, n_channels, n_samples = rawdata.shape\n",
    "            \n",
    "            print(f\"  Shape: {rawdata.shape}\")\n",
    "            print(f\"  Trials: {n_trials}\")\n",
    "            print(f\"  Channels: {n_channels}\")\n",
    "            print(f\"  Samples per trial: {n_samples}\")\n",
    "            print(f\"  Labels: {labels}\")\n",
    "            \n",
    "            return {\n",
    "                'data': rawdata,\n",
    "                'labels': labels,\n",
    "                'subject_id': subject_id,\n",
    "                'n_trials': n_trials,\n",
    "                'n_channels': n_channels,\n",
    "                'n_samples': n_samples,\n",
    "                'sampling_rate': 500  # From paper\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: 'rawdata' not found in .mat file\")\n",
    "            print(f\"Available keys: {list(mat_data.keys())}\")\n",
    "            return mat_data\n",
    "    \n",
    "    def load_preprocessed_edf_data(self, subject_id, preload=True):\n",
    "        \"\"\"Load paper's preprocessed .edf data\"\"\"\n",
    "        files = self.get_subject_files(subject_id, 'paper_preprocessed')\n",
    "        \n",
    "        if len(files) == 0:\n",
    "            raise ValueError(f\"No preprocessed data found for subject {subject_id}\")\n",
    "        \n",
    "        file_path = files.iloc[0]['file_path']\n",
    "        \n",
    "        print(f\"Loading preprocessed .edf data: {subject_id}\")\n",
    "        raw = mne.io.read_raw_edf(file_path, preload=preload, verbose=False)\n",
    "        \n",
    "        # Get data\n",
    "        if preload:\n",
    "            data, times = raw[:]\n",
    "        else:\n",
    "            data, times = None, None\n",
    "        \n",
    "        print(f\"  Channels: {len(raw.ch_names)}\")\n",
    "        print(f\"  Sampling rate: {raw.info['sfreq']} Hz\")\n",
    "        print(f\"  Duration: {raw.times[-1]:.2f} seconds\" if hasattr(raw, 'times') and len(raw.times) > 0 else \"\")\n",
    "        \n",
    "        return {\n",
    "            'raw': raw,\n",
    "            'data': data,\n",
    "            'times': times,\n",
    "            'subject_id': subject_id,\n",
    "            'channels': raw.ch_names,\n",
    "            'sampling_rate': raw.info['sfreq'],\n",
    "            'has_annotations': hasattr(raw, 'annotations') and bool(raw.annotations)\n",
    "        }\n",
    "    \n",
    "    def get_patient_info(self, subject_id):\n",
    "        \"\"\"Get patient information\"\"\"\n",
    "        # Extract subject number\n",
    "        try:\n",
    "            subject_num = int(subject_id.split('-')[1])\n",
    "        except:\n",
    "            subject_num = None\n",
    "        \n",
    "        # Find in participants\n",
    "        info = self.participants[self.participants['Participant_ID'] == subject_id]\n",
    "        \n",
    "        if len(info) > 0:\n",
    "            patient_info = info.iloc[0].to_dict()\n",
    "            patient_info['subject_num'] = subject_num\n",
    "            return patient_info\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_events_for_subject(self, subject_id):\n",
    "        \"\"\"Get events for a subject\"\"\"\n",
    "        if self.events is None:\n",
    "            return None\n",
    "        \n",
    "        # Extract subject number from subject_id\n",
    "        subject_num = int(subject_id.split('-')[1])\n",
    "        \n",
    "        # Events might be indexed by subject\n",
    "        # This depends on the events file structure\n",
    "        if 'participant_id' in self.events.columns:\n",
    "            subject_events = self.events[self.events['participant_id'] == subject_id]\n",
    "        else:\n",
    "            # Assume all events are in one file, need to separate by trial counts\n",
    "            # 40 trials per subject, so subject 1 = trials 0-39, subject 2 = trials 40-79, etc.\n",
    "            trials_per_subject = 40\n",
    "            start_trial = (subject_num - 1) * trials_per_subject\n",
    "            end_trial = start_trial + trials_per_subject\n",
    "            \n",
    "            if 'trial_id' in self.events.columns:\n",
    "                subject_events = self.events[\n",
    "                    (self.events['trial_id'] >= start_trial) & \n",
    "                    (self.events['trial_id'] < end_trial)\n",
    "                ]\n",
    "            else:\n",
    "                # Can't separate by subject\n",
    "                subject_events = self.events\n",
    "        \n",
    "        return subject_events\n",
    "    \n",
    "    def list_all_subjects(self):\n",
    "        \"\"\"List all available subjects\"\"\"\n",
    "        return sorted(self.file_index['subject_id'].unique())\n",
    "    \n",
    "    def get_subjects_with_both_data_types(self):\n",
    "        \"\"\"Get subjects that have both raw and preprocessed data\"\"\"\n",
    "        subject_counts = self.file_index.groupby('subject_id')['data_type'].nunique()\n",
    "        return subject_counts[subject_counts >= 2].index.tolist()\n",
    "    \n",
    "    def compare_data_types(self, subject_id):\n",
    "        \"\"\"Compare raw and preprocessed data for a subject\"\"\"\n",
    "        print(f\"\\\\n=== Comparing data for {subject_id} ===\")\n",
    "        \n",
    "        # Get file info\n",
    "        raw_files = self.get_subject_files(subject_id, 'raw')\n",
    "        preprocessed_files = self.get_subject_files(subject_id, 'paper_preprocessed')\n",
    "        \n",
    "        print(f\"Raw files: {len(raw_files)}\")\n",
    "        print(f\"Preprocessed files: {len(preprocessed_files)}\")\n",
    "        \n",
    "        if len(raw_files) > 0 and len(preprocessed_files) > 0:\n",
    "            # Load small sample to compare\n",
    "            try:\n",
    "                raw_data = self.load_raw_mat_data(subject_id)\n",
    "                preprocessed_data = self.load_preprocessed_edf_data(subject_id, preload=False)\n",
    "                \n",
    "                print(f\"\\\\nComparison:\")\n",
    "                print(f\"  Raw: {raw_data['n_trials']} trials, {raw_data['n_channels']} channels\")\n",
    "                print(f\"  Preprocessed: {len(preprocessed_data['channels'])} channels\")\n",
    "                \n",
    "                # Check if C3 and C4 are present in preprocessed data\n",
    "                if preprocessed_data['raw']:\n",
    "                    channels = preprocessed_data['channels']\n",
    "                    has_c3 = 'C3' in channels\n",
    "                    has_c4 = 'C4' in channels\n",
    "                    print(f\"  C3 in preprocessed: {has_c3}\")\n",
    "                    print(f\"  C4 in preprocessed: {has_c4}\")\n",
    "                \n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error comparing: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"Missing one or both data types\")\n",
    "            return False\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize loader\n",
    "    loader = EEGDatasetLoader()\n",
    "    \n",
    "    # List subjects\n",
    "    subjects = loader.list_all_subjects()[:5]\n",
    "    print(\"First 5 subjects:\", subjects)\n",
    "    \n",
    "    # Get subjects with both data types\n",
    "    complete_subjects = loader.get_subjects_with_both_data_types()\n",
    "    print(f\"\\\\nSubjects with both raw and preprocessed data: {len(complete_subjects)}\")\n",
    "    \n",
    "    # Load and compare data for first subject\n",
    "    if len(complete_subjects) > 0:\n",
    "        test_subject = complete_subjects[0]\n",
    "        loader.compare_data_types(test_subject)\n",
    "        \n",
    "        # Load raw data\n",
    "        print(f\"\\\\n--- Loading raw data for {test_subject} ---\")\n",
    "        raw_data = loader.load_raw_mat_data(test_subject)\n",
    "        \n",
    "        # Load preprocessed data\n",
    "        print(f\"\\\\n--- Loading preprocessed data for {test_subject} ---\")\n",
    "        preprocessed_data = loader.load_preprocessed_edf_data(test_subject, preload=False)\n",
    "        \n",
    "        # Get patient info\n",
    "        print(f\"\\\\n--- Patient info for {test_subject} ---\")\n",
    "        patient_info = loader.get_patient_info(test_subject)\n",
    "        if patient_info:\n",
    "            print(f\"Age: {patient_info.get('Age', 'N/A')}\")\n",
    "            print(f\"Gender: {patient_info.get('Gender', 'N/A')}\")\n",
    "            print(f\"Paralysis side: {patient_info.get('ParalysisSide', 'N/A')}\")\n",
    "'''\n",
    "\n",
    "loader_script_path = os.path.join(organized_dir, 'code', 'data_loader.py')\n",
    "with open(loader_script_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(loader_script_content)\n",
    "\n",
    "print(f\"âœ“ Data loader script created at: {loader_script_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imenv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
